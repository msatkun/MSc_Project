{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49f8abdb"
      },
      "source": [
        "# Individual Project (MSc) - Techniques for Compressing Pruned Feedforward Neural Networks: A Comparative Study\n",
        "\n",
        "**By Melany Satkunarajah** (S210010900), melany.satkunarajah@city.ac.uk\n",
        "\n",
        "This file has the full code for the project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jypLrapmMIdH"
      },
      "source": [
        "## Importing Relevant Modules\n",
        "Importing the relevant modules that are required for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4376c046"
      },
      "outputs": [],
      "source": [
        "#mount to google drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqMqhL6ct95r"
      },
      "outputs": [],
      "source": [
        "#import required modules\n",
        "import os\n",
        "import re\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4K-49viMKgF"
      },
      "source": [
        "## UC Irvine Machine Learning Repository Datasets\n",
        "\n",
        "In this section, we are going to review/prepare the dataset for our dimensionality reduction task, and split it into training and testing sets.\n",
        "\n",
        "\n",
        "The dataset was obtained from UC Irvine Machine Learning Repository:\n",
        "\n",
        "*   Online News Popularity - https://archive.ics.uci.edu/dataset/332/online+news+popularity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZLrFZC7iuwX"
      },
      "source": [
        "### Review/Preprocessing of the Dataset\n",
        "In this section, we are going to review and prepare the dataset for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OatLVoOMgST"
      },
      "outputs": [],
      "source": [
        "#import csv file using pandas\n",
        "#data_one = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/IP_MSC/UCI_code/datasets/OnlineNewsPopularity.csv', encoding='utf-8')\n",
        "!wget \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\"\n",
        "!unzip online+news+popularity.zip\n",
        "!ls OnlineNewsPopularity\n",
        "data_one = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-HnBm7KGPAk"
      },
      "outputs": [],
      "source": [
        "# Displaying the first few rows of the DataFrame\n",
        "data_one.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9DqMxxnGyzV"
      },
      "outputs": [],
      "source": [
        "# Displaying the text representation of the DataFrame shape\n",
        "print(\"Data_One Shape: ({} rows, {} columns)\".format(data_one.shape[0],\n",
        "                                                      data_one.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xJYYoHAHfle"
      },
      "outputs": [],
      "source": [
        "# Display concise information about the Dataset\n",
        "data_one.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsdxQjwkYCzo"
      },
      "source": [
        "Removing the leading spaces from the feature names as we can see output above that there are additional leading spaces in front of the feature names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJhsar64XeuD"
      },
      "outputs": [],
      "source": [
        "# Checking and remoivng leading spaces from feature names\n",
        "data_one.columns = data_one.columns.str.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34qpA8Ff4Ve5"
      },
      "source": [
        "Here is a full list of feature attributes with details of this dataset:\n",
        "1.   **url** -  URL of the article (non-predictive)\n",
        "2.   **timedelta** - Days between the article publication and the dataset acquisition (non-predictive)\n",
        "3.   **n_tokens_title** -  Number of words in the title\n",
        "4.   **n_tokens_content** - Number of words in the content\n",
        "5. **n_unique_tokens** - Rate of unique words in the content\n",
        "6. **n_non_stop_words** - Rate of non-stop words in the content\n",
        "7. **n_non_stop_unique_tokens** - Rate of unique non-stop words in the content\n",
        "8. **num_hrefs** - Number of links\n",
        "9. **num_self_hrefs** - Number of links to other articles published by Mashable\n",
        "10. **num_imgs** - Number of images\n",
        "11. **num_videos** - Number of videos\n",
        "12. **average_token_length** - Average length of the words in the content\n",
        "13. **num_keywords** - Number of keywords in the metadata\n",
        "14. **data_channel_is_lifestyle** - Is data channel 'Lifestyle'?\n",
        "15. **data_channel_is_entertainment** - Is data channel 'Entertainment'?\n",
        "16. **data_channel_is_bus** - Is data channel 'Business'?\n",
        "17. **data_channel_is_socmed** - Is data channel 'Social Media'?\n",
        "18. **data_channel_is_tech** - Is data channel 'Tech'?\n",
        "19. **data_channel_is_world** - Is data channel 'World'?\n",
        "20. **kw_min_min** - Worst keyword (min. shares)\n",
        "21. **kw_max_min** - Worst keyword (max. shares)\n",
        "22. **kw_avg_min** - Worst keyword (avg. shares)\n",
        "23. **kw_min_max** - Best keyword (min. shares)\n",
        "24. **kw_max_max** - Best keyword (max. shares)\n",
        "25. **kw_avg_max** - Best keyword (avg. shares)\n",
        "26. **kw_min_avg** - Avg. keyword (min. shares)\n",
        "27. **kw_max_avg** - Avg. keyword (max. shares)\n",
        "28. **kw_avg_avg** - Avg. keyword (avg. shares)\n",
        "29. **self_reference_min_shares** - Min. shares of referenced articles in Mashable\n",
        "30. **self_reference_max_shares** - Max. shares of referenced articles in Mashable\n",
        "31. **self_reference_avg_sharess** - Avg. shares of referenced articles in Mashable\n",
        "32. **weekday_is_monday** - Was the article published on a Monday?\n",
        "33. **weekday_is_tuesday** - Was the article published on a Tuesday?\n",
        "34. **weekday_is_wednesday** - Was the article published on a Wednesday?\n",
        "35. **weekday_is_thursday** - Was the article published on a Thursday?\n",
        "36. **weekday_is_friday** - Was the article published on a Friday?\n",
        "37. **weekday_is_saturday** - Was the article published on a Saturday?\n",
        "38. **weekday_is_sunday** - Was the article published on a Sunday?\n",
        "39. **is_weekend** - Was the article published on the weekend?\n",
        "40. **LDA_00** - Closeness to LDA topic 0\n",
        "41. **LDA_01** - Closeness to LDA topic 1\n",
        "42. **LDA_02** - Closeness to LDA topic 2\n",
        "43. **LDA_03** - Closeness to LDA topic 3\n",
        "44. **LDA_04** - Closeness to LDA topic 4\n",
        "45. **global_subjectivity** - Text subjectivity\n",
        "46. **global_sentiment_polarity** - Text sentiment polarity\n",
        "47. **global_rate_positive_words** - Rate of positive words in the content\n",
        "48. **global_rate_negative_words** - Rate of negative words in the content\n",
        "49. **rate_positive_words** - Rate of positive words among non-neutral tokens\n",
        "50. **rate_negative_words** - Rate of negative words among non-neutral tokens\n",
        "51. **avg_positive_polarity** - Avg. polarity of positive words\n",
        "52. **min_positive_polarity** - Min. polarity of positive words\n",
        "53. **max_positive_polarity** - Max. polarity of positive words\n",
        "54. **avg_negative_polarity** - Avg. polarity of negative  words\n",
        "55. **min_negative_polarity** - Min. polarity of negative  words\n",
        "56. **max_negative_polarity** - Max. polarity of negative  words\n",
        "57. **title_subjectivity** - Title subjectivity\n",
        "58. **title_sentiment_polarity** - Title polarity\n",
        "59. **abs_title_subjectivity** - Absolute subjectivity level\n",
        "60. **abs_title_sentiment_polarity** - Absolute polarity level\n",
        "61. **shares** - Number of shares (target)\n",
        "\n",
        "Reference for obtaining feature attribute description:\n",
        "https://archive.ics.uci.edu/dataset/332/online+news+popularity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMlbZLqnIJkr"
      },
      "outputs": [],
      "source": [
        "# Checking for missing values\n",
        "missing_values = data_one.isnull().sum()\n",
        "\n",
        "# Displaying the count of missing values for each column\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Checking for duplicate rows\n",
        "duplicate_rows = data_one.duplicated().sum()\n",
        "\n",
        "# Displaying the count of duplicate rows\n",
        "print(\"\\nDuplicate Rows:\", duplicate_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfHcHVQInAK"
      },
      "source": [
        "\n",
        "\n",
        "*   No duplicate or missing values in this dataset\n",
        "\n",
        "After reviewing the columns, it is ideal to drop the non-predictive attricbutes, which is in our case:\n",
        "*   url\n",
        "*   timedelta\n",
        "\n",
        "The non-predictive attributes are listed here: https://archive.ics.uci.edu/dataset/332/online+news+popularity\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6hggJquWkuH"
      },
      "outputs": [],
      "source": [
        "# Remove specified columns from the DataFrame\n",
        "columns_to_drop = ['url', 'timedelta']\n",
        "data_one.drop(columns=columns_to_drop, inplace=True)\n",
        "data_one.head(n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtlcUCaubnJn"
      },
      "source": [
        "These are the binary columns in this dataset:\n",
        "*   data_channel_is_lifestyle\n",
        "*   data_channel_is_entertainment\n",
        "*   data_channel_is_bus\n",
        "*   data_channel_is_socmed\n",
        "*   data_channel_is_tech\n",
        "*   data_channel_is_world\n",
        "*   weekday_is_monday\n",
        "*   weekday_is_tuesday\n",
        "*   weekday_is_wednesday\n",
        "*   weekday_is_thursday\n",
        "*   weekday_is_friday\n",
        "*   weekday_is_saturday\n",
        "*   weekday_is_sunday\n",
        "*   is_weekend\n",
        "\n",
        "We have changed the type from float to boolean, so it gives us a true or false output instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4vrqibyal7H"
      },
      "outputs": [],
      "source": [
        "# Identify binary columns\n",
        "binary_columns = [col for col in data_one.columns if data_one[col].nunique() == 2]\n",
        "\n",
        "# Display the binary columns\n",
        "print(\"Binary Columns:\")\n",
        "print(binary_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTodrL5GbOvT"
      },
      "outputs": [],
      "source": [
        "# Convert binary columns to boolean type\n",
        "data_one[binary_columns] = data_one[binary_columns].astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRwQa87_cdY9"
      },
      "outputs": [],
      "source": [
        "# check if the changes have been applied in a random column\n",
        "data_one['weekday_is_friday']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6o6VVFOefV7"
      },
      "outputs": [],
      "source": [
        "# Display the first 4 rows of the modified DataFrame\n",
        "data_one.head(n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqXvUC_retEn"
      },
      "source": [
        "Reviewing the table above, you can see that the following features have discrete values:\n",
        "*   n_tokens_title\n",
        "*   n_tokens_content\n",
        "*   num_hrefs\n",
        "*   num_self_hrefs\n",
        "*   num_imgs\n",
        "*   num_videos\n",
        "*   num_keywords\n",
        "\n",
        "Instead of saving them as floats, we will convert them to an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zKuyPXifHPj"
      },
      "outputs": [],
      "source": [
        "# list of feature attributes that have discrete values\n",
        "integer_features = ['n_tokens_title', 'n_tokens_content', 'num_hrefs',\n",
        "                    'num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords']\n",
        "\n",
        "# changing these features into integer values\n",
        "data_one[integer_features] = data_one[integer_features].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7B1bl0ffjp6"
      },
      "outputs": [],
      "source": [
        "# checking the columns after amendents made\n",
        "print(data_one.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8InLnHOgsKF"
      },
      "source": [
        "After we have made the changes for the feature types, and removed the two non-predictive columns. Let's look at the descriptive statistical measure for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcxJYSmNgom8"
      },
      "outputs": [],
      "source": [
        "# Displaying descriptive statistics of the DataFrame\n",
        "data_one.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSAEoabyhGaN"
      },
      "outputs": [],
      "source": [
        "# Current DataFrame shape\n",
        "print(\"Data_One Shape: ({} rows, {} columns)\".format(data_one.shape[0],\n",
        "                                                      data_one.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4cDDzc0gglB"
      },
      "source": [
        "**'n_tokens_content'** represents the number of words in the news articles and some some articles have inputs with a minimum value of 0 indicate articles with no content which add no meaning for sharing activity of the article. So we are going to remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh6vCqII7R_h"
      },
      "outputs": [],
      "source": [
        "# Identify rows where 'n_tokens_content' is 0\n",
        "no_words_indices = data_one[data_one['n_tokens_content'] == 0].index\n",
        "\n",
        "# Print the number of news items with no words\n",
        "print('Number of News Articles with no words:', no_words_indices.size)\n",
        "\n",
        "# Drop rows where 'n_tokens_content' is 0\n",
        "data_one = data_one[data_one['n_tokens_content'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK6LEysAg6NC"
      },
      "outputs": [],
      "source": [
        "# Current DataFrame shape\n",
        "print(\"Data_One Shape: ({} rows, {} columns)\".format(data_one.shape[0],\n",
        "                                                      data_one.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKvOGORO1TVK"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data_one.corr()\n",
        "\n",
        "# Create a heatmap using seaborn to visualize the correlation matrix\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\")\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiFhVCeBhanw"
      },
      "outputs": [],
      "source": [
        "# Displaying histograms for each numerical column in the DataFrame\n",
        "data_one.hist(figsize=(20, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-giJd2xd1oN"
      },
      "source": [
        "**Target Value Distribution**\n",
        "\n",
        "\n",
        "The target variable of this dataset is the 'shares' column, which is the number of shares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EtSCVKhYxSY"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Set the plotting style\n",
        "plt.style.use('default')\n",
        "\n",
        "# Create a subplot within the figure\n",
        "plt.subplot()\n",
        "\n",
        "# Plot a histogram for the 'shares' column\n",
        "data_one['shares'].hist(bins=10, range=(1, 8000))\n",
        "\n",
        "# Set the title and axis labels for the plot\n",
        "plt.title('Histogram of Shares of Online Articles')\n",
        "plt.xlabel('Shares')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRvjo_K270PO"
      },
      "outputs": [],
      "source": [
        "# output target variable distribution\n",
        "data_one['shares'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI5VsvRFn3qu"
      },
      "source": [
        "To determine if an article is popular, we look at how many times it has been shared online. Using the median number of shares in the dataset as a benchmark.\n",
        "\n",
        "If an article has been shared 1400 or more times, it is considered popular.\n",
        "\n",
        "If an article has been shared less than 1400 times, it is considered less popular.\n",
        "\n",
        "We will create a new column, popularity, to determine the popularity of the article based on the number of shares, and then we can do that throughout the week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLz6j9md0rMx"
      },
      "outputs": [],
      "source": [
        "# checking the numbers of words in the title and content of the article\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(2)\n",
        "\n",
        "# Plot a boxplot for 'n_tokens_title' in the first subplot\n",
        "sns.boxplot(data=data_one, x='n_tokens_title', ax=axs[0])\n",
        "\n",
        "# Plot a boxplot for 'n_tokens_content' in the second subplot\n",
        "sns.boxplot(data=data_one, x='n_tokens_content', ax=axs[1])\n",
        "\n",
        "# title\n",
        "fig.suptitle('Number of Words in Title against Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCRQoh-GSWCB"
      },
      "source": [
        "#### Identifying and Removing Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5vMJWoqWDbB"
      },
      "outputs": [],
      "source": [
        "# Get numerical columns within the dataset\n",
        "num_cols = data_one.select_dtypes(['int64', 'float64']).columns\n",
        "\n",
        "# Calculate the number of rows and columns for the grid\n",
        "num_plots = len(num_cols)\n",
        "num_rows = math.ceil(num_plots / 3)\n",
        "num_cols_subplot = min(num_plots, 3)\n",
        "\n",
        "# Set up the grid for boxplots\n",
        "fig, axs = plt.subplots(num_rows, num_cols_subplot, figsize=(20, num_rows * 4))\n",
        "\n",
        "# Flattening the grid\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Plot boxplots in the grid\n",
        "for i, column in enumerate(num_cols):\n",
        "    sns.boxplot(x=data_one[column], ax=axs[i])\n",
        "    axs[i].set_title(f'Boxplot of {column}')\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(num_plots, len(axs)):\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRqytUg6V-1Z"
      },
      "outputs": [],
      "source": [
        "# Identify and print outliers for each numerical column\n",
        "for column in num_cols:\n",
        "    q1 = data_one[column].quantile(0.25)  # First Quartile\n",
        "    q3 = data_one[column].quantile(0.75)  # Third Quartile\n",
        "    IQR = q3 - q1  # Interquartile Range\n",
        "\n",
        "    llimit = q1 - 1.5 * IQR  # Lower Limit\n",
        "    ulimit = q3 + 1.5 * IQR  # Upper Limit\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = data_one[(data_one[column] < llimit) | (data_one[column] > ulimit)]\n",
        "\n",
        "    # Print information about outliers\n",
        "    print(f'Column: {column}')\n",
        "    print(f'Number of outliers in \"{column}\": {len(outliers)}')\n",
        "    print(f'Lower Limit: {llimit}')\n",
        "    print(f'Upper Limit: {ulimit}')\n",
        "    print(f'Interquartile Range (IQR): {IQR}')\n",
        "    print('-' * 30)  # Separator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsAoBi5Pb2mJ"
      },
      "outputs": [],
      "source": [
        "# Createing a copy of the original dataset\n",
        "data_one_no_outliers = data_one.copy()\n",
        "\n",
        "# Identify and remove outliers for each numerical column\n",
        "for column in num_cols:\n",
        "    q1 = data_one[column].quantile(0.25)  # First Quartile\n",
        "    q3 = data_one[column].quantile(0.75)  # Third Quartile\n",
        "    IQR = q3 - q1  # Interquartile Range\n",
        "\n",
        "    llimit = q1 - 1.5 * IQR  # Lower Limit\n",
        "    ulimit = q3 + 1.5 * IQR  # Upper Limit\n",
        "\n",
        "    # Remove outliers and update the copy\n",
        "    data_one_no_outliers = data_one_no_outliers[\n",
        "        (data_one_no_outliers[column] >= llimit) & (data_one_no_outliers[column] <= ulimit)\n",
        "    ]\n",
        "\n",
        "# Display the shape of the original and outlier-removed datasets\n",
        "print(\"Original Dataset Shape:\", data_one.shape)\n",
        "print(\"Outlier-Removed Dataset Shape:\", data_one_no_outliers.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCgWB6huDfEU"
      },
      "outputs": [],
      "source": [
        "# Current DataFrame shape after removing outliers\n",
        "print(\"Data_One Shape after Removing Outliers: ({} rows, {} columns)\".format(data_one_no_outliers.shape[0],\n",
        "                                                                             data_one_no_outliers.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQL1Gf6j4kVh"
      },
      "outputs": [],
      "source": [
        "data_one_no_outliers.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMkm2a7ci9b1"
      },
      "source": [
        "### Splitting into Training and Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow6KyDD0aFwe"
      },
      "source": [
        "**Dataset One with Outliers**\n",
        "\n",
        "This is the original dataset including the outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoV7nxUHqtEd"
      },
      "outputs": [],
      "source": [
        "# features are stored in X and the target ('shares') is stored in y\n",
        "X0 = data_one.drop('shares', axis=1)  # Features\n",
        "y0 = data_one['shares']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0, test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"X0_train shape:\", X0_train.shape)\n",
        "print(\"X0_test shape:\", X0_test.shape)\n",
        "print(\"y0_train shape:\", y0_train.shape)\n",
        "print(\"y0_test shape:\", y0_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlfJy1zcaQG6"
      },
      "source": [
        "**Dataset_One without Outliers**\n",
        "\n",
        "This is the original dataset excluding the outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENM2VM1b7JyI"
      },
      "outputs": [],
      "source": [
        "# features are stored in X and the target ('shares') is stored in y\n",
        "X = data_one_no_outliers.drop('shares', axis=1)  # Features\n",
        "y = data_one_no_outliers['shares']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZwoq_31qzNq"
      },
      "source": [
        "## Feedforward Neural Network for Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xtaD3UchxGl"
      },
      "source": [
        "Below we have defined the feedforward neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-WTJ8lrR5rO"
      },
      "outputs": [],
      "source": [
        "class RegressionNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(RegressionNN, self).__init__()\n",
        "\n",
        "        # Define fully connected layers with ReLU activations\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "\n",
        "        # Initialize the weights using Xavier initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Xavier initialization for each layer\n",
        "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
        "            nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# reference used: https://www.geeksforgeeks.org/select-the-right-weight-for-deep-neural-network-in-pytorch/\n",
        "# https://pytorch.org/docs/stable/nn.init.html\n",
        "# https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/#:~:text=If%20your%20problem%20is%20a,%3A%20One%20node%2C%20linear%20activation.\n",
        "# https://medium.com/@gidim/part-2-selecting-the-right-weight-initialization-for-your-deep-neural-network-cc27cf2d5e56"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4x7dcBj9GSS"
      },
      "source": [
        "**Moving the model to the specified device (GPU or CPU)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr31zOgFGB19"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "# ref: https://stackoverflow.com/questions/50560395/how-to-install-cuda-in-google-colab-gpus\n",
        "# Output would be True if Pytorch is using GPU otherwise it would be False.\n",
        "\n",
        "# defining training device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZkDB-f6HxA"
      },
      "source": [
        "### Scaling the features\n",
        "\n",
        "Before we train the RegressionNN network that we have defined above, we need to double check the types of our training and testing sets. So we can convert and scale them appropriately for our regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGpIB00cBcC9"
      },
      "outputs": [],
      "source": [
        "# Checking the type of training and testing set\n",
        "print(\"Types for Dataset with outliers\")\n",
        "print(type(y0_test))\n",
        "print(type(y0_train))\n",
        "print(type(X0_train))\n",
        "print(type(X0_test))\n",
        "print()\n",
        "print(\"Types for Dataset without outliers\")\n",
        "print(type(y_test))\n",
        "print(type(y_train))\n",
        "print(type(X_train))\n",
        "print(type(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoVdIRY1K5aO"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(X_train, X_test, y_train, y_test, batch_size=64, device='cpu'):\n",
        "    # Step 1: Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Step 2: Converting to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1).to(device)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1).to(device)\n",
        "\n",
        "    # Step 3: Createing DataLoader for training data\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Step 4: Creating DataLoader for testing data\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6b5RAm67Pfd"
      },
      "outputs": [],
      "source": [
        "# Using the preprocess_data function on the dataset with outliers\n",
        "train_loader0, test_loader0 = preprocess_data(X0_train, X0_test,y0_train, y0_test,\n",
        "                                              batch_size=64,\n",
        "                                              device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Print the shapes of tensors\n",
        "for batch in train_loader0:\n",
        "    X0_batch, y0_batch = batch\n",
        "    print(\"X0_batch shape:\", X0_batch.shape)\n",
        "    print(\"y0_batch shape:\", y0_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnBNpl9I7kjP"
      },
      "outputs": [],
      "source": [
        "# Using the preprocess_data function on the dataset without outliers\n",
        "train_loader, test_loader = preprocess_data(X_train, X_test, y_train, y_test,\n",
        "                                            batch_size=64,\n",
        "                                            device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Print the shapes of tensors\n",
        "for batch in train_loader:\n",
        "    X_batch, y_batch = batch\n",
        "    print(\"X_batch shape:\", X_batch.shape)\n",
        "    print(\"y_batch shape:\", y_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeJ5eknKMrxs"
      },
      "outputs": [],
      "source": [
        "# PCA on the dataset without outliers\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components = np.argmax(cumulative_variance >= 0.95) + 1  # 95% threshold\n",
        "\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         cumulative_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAPGT4OPq-Hn"
      },
      "source": [
        "## Training and Testing of the Feedford Neural Network\n",
        "\n",
        "Before we train the RegressionNN network that we have defined above, we need to double check the types of our training and testing sets. So we can convert them appropriately for our regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0nz8vLQBkMN"
      },
      "outputs": [],
      "source": [
        "# reference: https://discuss.pytorch.org/t/output-evaluation-loss-after-every-n-batches-instead-of-epochs-with-pytorch/116619\n",
        "def OG_train_and_validate(train_loader, test_loader, input_size, hidden_size1, hidden_size2,\n",
        "                       output_size, learning_rate, epochs):\n",
        "    # Creating an object of the RegressionNN class\n",
        "    regression_net = RegressionNN(input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size)\n",
        "\n",
        "    # Move the model to CUDA if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    regression_net = regression_net.to(device)\n",
        "\n",
        "    # Defining Mean Squared Error loss function\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Defining the Adam optimizer\n",
        "    optimizer = torch.optim.Adam(regression_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # List to store training and validation losses\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Set the model to training mode\n",
        "        regression_net.train()\n",
        "\n",
        "        # Initializing total loss and total MAE\n",
        "        total_loss = 0.0\n",
        "        total_mae = 0.0\n",
        "\n",
        "        # List to store all outputs during training\n",
        "        all_outputs = []\n",
        "\n",
        "        # Iterating over training data in batches\n",
        "        for batch in train_loader:\n",
        "            inputs = batch[0].to(device)\n",
        "            targets = batch[1].squeeze().to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = regression_net(inputs)\n",
        "\n",
        "            # Calculating the MSE loss\n",
        "            mse_loss = criterion(outputs.view(-1), targets.view(-1))\n",
        "\n",
        "            # Calculating the MAE loss\n",
        "            mae_loss = nn.L1Loss()(outputs.view(-1), targets.view(-1))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            mse_loss.backward()\n",
        "\n",
        "            # Store outputs for later printing\n",
        "            all_outputs.extend(outputs.view(-1).tolist())\n",
        "\n",
        "            # Gradient Clipping\n",
        "            # reference used: https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
        "            torch.nn.utils.clip_grad_norm_(regression_net.parameters(),\n",
        "                                           max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update total loss\n",
        "            total_loss += mse_loss.item()\n",
        "\n",
        "            # Update total MAE\n",
        "            total_mae += mae_loss.item()\n",
        "\n",
        "        # Calculating average loss and MAE for the epoch\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        average_mae = total_mae / len(train_loader)\n",
        "\n",
        "        # Storing training loss\n",
        "        training_losses.append(average_loss)\n",
        "\n",
        "        # Printing the metrics for each epoch\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], MSE Loss (Train): {average_loss:.3f}, MAE Loss (Train): {average_mae:.3f}')\n",
        "\n",
        "        # Validation\n",
        "        regression_net.eval()\n",
        "        with torch.no_grad():\n",
        "            validation_loss, validation_mae = 0.0, 0.0\n",
        "\n",
        "            # Iterate over validation data in batches\n",
        "            for batch in test_loader:\n",
        "                val_inputs = batch[0].to(device)\n",
        "                val_targets = batch[1].squeeze().to(device)\n",
        "\n",
        "                val_outputs = regression_net(val_inputs)\n",
        "                validation_loss += criterion(val_outputs.view(-1),\n",
        "                                             val_targets.view(-1)).item()\n",
        "                validation_mae += nn.L1Loss()(val_outputs.view(-1),\n",
        "                                              val_targets.view(-1)).item()\n",
        "\n",
        "            # Calculate average validation loss and MAE\n",
        "            average_validation_loss = validation_loss / len(test_loader)\n",
        "            average_validation_mae = validation_mae / len(test_loader)\n",
        "\n",
        "            # Store validation loss for later plotting\n",
        "            validation_losses.append(average_validation_loss)\n",
        "\n",
        "            # Print the validation metrics\n",
        "            print(f'Validation Loss: {average_validation_loss:.3f}, Validation MAE: {average_validation_mae:.3f}')\n",
        "\n",
        "    # Plotting\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    plt.plot(epochs_range, training_losses, label='Training Loss')\n",
        "    plt.plot(epochs_range, validation_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return regression_net, training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0rkYyfZJ8zb"
      },
      "source": [
        "References used for the training and testing loop above:\n",
        "\n",
        "https://discuss.pytorch.org/t/output-evaluation-loss-after-every-n-batches-instead-of-epochs-with-pytorch/116619"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UUXsfC8qOd5"
      },
      "outputs": [],
      "source": [
        "# function to do grid search\n",
        "# rerence: https://towardsdatascience.com/how-to-write-your-grid-search-function-in-python-43ad0da97522\n",
        "def grid_search(train_loader, test_loader, input_size, output_size,\n",
        "                learning_rates, batch_sizes, hidden_sizes1, hidden_sizes2, epochs):\n",
        "\n",
        "    # Initialize variables to store the best model, its loss, and information\n",
        "    best_model = None\n",
        "    best_loss = float('inf')\n",
        "    best_model_info = {}\n",
        "\n",
        "    # Iterate over hyperparameter combinations\n",
        "    for lr in learning_rates:\n",
        "        for batch_size in batch_sizes:\n",
        "            for hidden_size1 in hidden_sizes1:\n",
        "                for hidden_size2 in hidden_sizes2:\n",
        "                    print(f\"\\nTraining for lr={lr}, batch_size={batch_size}, hidden_size1={hidden_size1}, hidden_size2={hidden_size2}\")\n",
        "\n",
        "                    # Create and train the model\n",
        "                    model, training_losses, validation_losses = OG_train_and_validate(\n",
        "                        train_loader, test_loader, input_size,\n",
        "                        hidden_size1, hidden_size2, output_size,\n",
        "                        learning_rate=lr, epochs=epochs\n",
        "                    )\n",
        "\n",
        "                    # Checking if the current model has the lowest validation loss\n",
        "                    if validation_losses[-1] < best_loss:\n",
        "                        best_loss = validation_losses[-1]\n",
        "                        best_model = model\n",
        "\n",
        "                        # Save the best model's state_dict to a file\n",
        "                        torch.save(best_model.state_dict(), 'best_model.pth')\n",
        "\n",
        "                        # Save the hyperparameters of the best model\n",
        "                        best_model_info = {\n",
        "                            'input_size': input_size,\n",
        "                            'hidden_size1': hidden_size1,\n",
        "                            'hidden_size2': hidden_size2,\n",
        "                            'output_size': output_size,\n",
        "                            'learning_rate': lr,\n",
        "                            'epochs': epochs,\n",
        "                            'batch_size': batch_size\n",
        "                        }\n",
        "\n",
        "    # Displaying information about the best model\n",
        "    print(\"\\nBest Model Information:\")\n",
        "    print(f\"Input Size: {best_model_info['input_size']}\")\n",
        "    print(f\"Hidden Size 1: {best_model_info['hidden_size1']}\")\n",
        "    print(f\"Hidden Size 2: {best_model_info['hidden_size2']}\")\n",
        "    print(f\"Output Size: {best_model_info['output_size']}\")\n",
        "    print(f\"Learning Rate: {best_model_info['learning_rate']}\")\n",
        "    print(f\"Epochs: {best_model_info['epochs']}\")\n",
        "    print(f\"Batch Size: {best_model_info['batch_size']}\")\n",
        "    print(f\"Best Validation Loss: {best_loss:.3f}\")\n",
        "\n",
        "    # Return the best model and its information as a directory\n",
        "    return  {'best_model': best_model, 'best_model_info': best_model_info}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiX6LyfmU82"
      },
      "source": [
        "#### Dataset One with Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcBAP2S-fuZS"
      },
      "outputs": [],
      "source": [
        "# Set your input sizes\n",
        "input_size = X0_train.shape[1]  # Number of input features\n",
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "output_size = 1  # For regression\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Call train_and_validate function for the dataset with outliers\n",
        "trained_model0, training_losses0, validation_losses0 = OG_train_and_validate(\n",
        "    train_loader0, test_loader0, input_size, hidden_size1, hidden_size2,\n",
        "    output_size,learning_rate=learning_rate, epochs=epochs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLmS-TPUmmOZ"
      },
      "source": [
        "#### Dataset One without Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUbh2ESVQBPS"
      },
      "outputs": [],
      "source": [
        "# Set your input sizes\n",
        "input_size = X_train.shape[1]  # Number of input features\n",
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "output_size = 1  # For regression\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Call train_and_validate function for the dataset wihtout outliers\n",
        "trained_model, training_losses, validation_losses = OG_train_and_validate(\n",
        "    train_loader, test_loader, input_size, hidden_size1, hidden_size2,\n",
        "    output_size,learning_rate=learning_rate, epochs=epochs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV71kIm4YKCA"
      },
      "source": [
        "##### Grid Search to find the best Model\n",
        "\n",
        "From this point onwards, we will use the dataset without outliers, as shown in the training above. This dataset had better training and validation losses than the dataset with outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6d5WW9jYKCR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for grid search\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [64, 128]\n",
        "hidden_sizes1 = [64, 128, 256]\n",
        "hidden_sizes2 = [32, 64, 128]\n",
        "epochs = 200\n",
        "\n",
        "# Perform grid search using the dataset without outliers\n",
        "best_model, best_model_info = grid_search(train_loader, test_loader,\n",
        "                                          input_size=X_train.shape[1],\n",
        "                                          output_size=1,\n",
        "                                          learning_rates=learning_rates,\n",
        "                                          batch_sizes=batch_sizes,\n",
        "                                          hidden_sizes1=hidden_sizes1,\n",
        "                                          hidden_sizes2=hidden_sizes2,\n",
        "                                          epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIgeIRnLmmOd"
      },
      "source": [
        "#### Saving the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OE_SJ3vmmOd"
      },
      "outputs": [],
      "source": [
        "# Save the best model's state_dict to Google Drive\n",
        "model_save_path_drive = '/content/gdrive/My Drive/Colab Notebooks/IP_MSC/UCI_code/best_model.pth'\n",
        "best_model_path = '/content/best_model.pth'\n",
        "!cp \"$best_model_path\" \"$model_save_path_drive\"\n",
        "\n",
        "# Save the best model info to Google Drive\n",
        "model_info_save_path_drive = '/content/gdrive/My Drive/Colab Notebooks/IP_MSC/UCI_code/best_model_info.json'\n",
        "with open(model_info_save_path_drive, 'w') as json_file:\n",
        "    json.dump(best_model_info, json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV54W5M3BMPe"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for best model\n",
        "input_size = X_train.shape[1]  # Number of input features\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32\n",
        "output_size = 1  # For regression\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0Kj8RjGBMPf"
      },
      "outputs": [],
      "source": [
        "# Call train_and_validate function to show the training of the best model\n",
        "# This is not being used for anything else but just for visualisation\n",
        "trained_model_b, training_losses_b, validation_losses_b = OG_train_and_validate(\n",
        "    train_loader, test_loader, input_size, hidden_size1, hidden_size2,\n",
        "    output_size,learning_rate=learning_rate, epochs=epochs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2e0reSHIqo-"
      },
      "source": [
        "## **Load the Trained Model**\n",
        "\n",
        "In this section, we are going to load the trained model and review quickly the number of parameters and the structure of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oUBqpMlfHCK"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for best model\n",
        "input_size = 58  # Number of input features\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32\n",
        "output_size = 1  # For regression\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyRRIrFP1_Y6"
      },
      "outputs": [],
      "source": [
        "#Load the saved model from google drive\n",
        "model = RegressionNN(input_size,hidden_size1, hidden_size2, output_size)\n",
        "model_path = '/content/gdrive/My Drive/Colab Notebooks/IP_MSC/UCI_code/best_model.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR8IrP7V0fYh"
      },
      "outputs": [],
      "source": [
        "# Apply training mode before applying pruning on the loaded model\n",
        "#model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PmmeiX_VmY5"
      },
      "outputs": [],
      "source": [
        "# Put the model in evaluation mode\n",
        "#model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dmTiJl8iBLi"
      },
      "outputs": [],
      "source": [
        "#function to obtain the model size\n",
        "# reference: https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n",
        "# https://wandb.ai/wandb_fc/tips/reports/How-To-Calculate-Number-of-Model-Parameters-for-PyTorch-and-TensorFlow-Models--VmlldzoyMDYyNzIx\n",
        "model_size = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Loaded model size: {model_size} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPA5XBolb9zv"
      },
      "source": [
        "The funcation, `check_model_stats`, below takes in a neural network model as input. It checks the sparsity of specific layers within the model, calculating the ratio of zero-valued weights to the total number of weights in each parameter tensor. The function then prints the layer name, parameter name, sparsity value, shape, and original weights of the parameter tensor. This information helps assess the impact of pruning techniques on the model's parameters, particularly regarding the sparsity introduced after pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1L2uKh8MdXQ"
      },
      "outputs": [],
      "source": [
        "# defining a function to check model stats for the orignal and pruned model\n",
        "def check_model_stats(model):\n",
        "    # Define global variables for parameters to check\n",
        "    parameters_to_check = [\n",
        "        (model.fc1, 'weight'),\n",
        "        (model.fc2, 'weight'),\n",
        "        (model.fc3, 'weight'),\n",
        "    ]\n",
        "\n",
        "    # Print sparsity and other information\n",
        "    print(\"Stats for each parameter:\")\n",
        "    for layer, parameter_name in parameters_to_check:\n",
        "        parameter_tensor = getattr(layer, parameter_name)\n",
        "        sparsity_value = torch.sum(parameter_tensor == 0).item() / parameter_tensor.numel()\n",
        "        print(f\"{layer.__class__.__name__}.{parameter_name}:\")\n",
        "        print(f\"Sparsity: {sparsity_value:.4f}\")\n",
        "        print(f\"Shape: {parameter_tensor.shape}\")\n",
        "        print(f\"Original weights:\")\n",
        "        print(parameter_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeR4BN2VMkZb"
      },
      "outputs": [],
      "source": [
        "check_model_stats(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V4mBO8YcYro"
      },
      "source": [
        "The function `get_zero_nonzero_values` analyzes the distribution of zero and non-zero values in the weight parameters of a neural network model. It extracts these values and updates counters, then prints the shape of each weight parameter and additional statistics such as minimum, maximum, mean, and standard deviation if there are non-zero values. Finally, the function prints the counts of zero and non-zero values and returns the lists containing these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRW9YV9kbFpI"
      },
      "outputs": [],
      "source": [
        "def get_zero_nonzero_values(model, epsilon=1e-8):\n",
        "    zero_values = []\n",
        "    nonzero_values = []\n",
        "\n",
        "    zero_count = 0\n",
        "    nonzero_count = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Assuming you are interested in pruning weights\n",
        "            # Convert the parameter tensor to a flat 1D tensor\n",
        "            flat_param = param.flatten()\n",
        "\n",
        "            # Use boolean masks to identify zero and non-zero values\n",
        "            zero_mask = (torch.abs(flat_param) <= epsilon)\n",
        "            nonzero_mask = ~zero_mask\n",
        "\n",
        "            # Extract zero and non-zero values using boolean masks\n",
        "            zero_values.extend(flat_param[zero_mask].tolist())\n",
        "            nonzero_values.extend(flat_param[nonzero_mask].tolist())\n",
        "\n",
        "            # Update counts\n",
        "            zero_count += torch.sum(zero_mask).item()\n",
        "            nonzero_count += torch.sum(nonzero_mask).item()\n",
        "\n",
        "            # Print shape of the parameter tensor\n",
        "            print(f\"Parameter '{name}' Shape: {param.shape}\")\n",
        "\n",
        "            # Print additional information about non-zero values\n",
        "            if torch.sum(nonzero_mask).item() > 0:\n",
        "                print(f\"Non-Zero Values ({name}):\")\n",
        "                print(f\"  Min: {torch.min(flat_param[nonzero_mask]).item()}\")\n",
        "                print(f\"  Max: {torch.max(flat_param[nonzero_mask]).item()}\")\n",
        "                print(f\"  Mean: {torch.mean(flat_param[nonzero_mask]).item()}\")\n",
        "                print(f\"  Std: {torch.std(flat_param[nonzero_mask]).item()}\\n\")\n",
        "\n",
        "    print(\"\\nCount of Zero Values:\", zero_count)\n",
        "    print(\"Count of Non-Zero Values:\", nonzero_count)\n",
        "\n",
        "    return zero_values, nonzero_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhDUhatUbIf8"
      },
      "outputs": [],
      "source": [
        "zero_values, nonzero_values = get_zero_nonzero_values(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELxKD_ERraG4"
      },
      "source": [
        "## Pruning of the Trained Feedforward Neural Network\n",
        "\n",
        "In this section, we are going to apply pruning on the already trained model and see how it affects the model when pruning is applied at different thresholds.\n",
        "\n",
        "References used:\n",
        "\n",
        "https://towardsdatascience.com/how-to-prune-neural-networks-with-pytorch-ebef60316b91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i05CgesFUeLE"
      },
      "source": [
        "### Global Sparsity Function\n",
        "\n",
        "The sparsity function calculates a PyTorch model's global sparsity by dividing the number of zero-valued parameters by the total number of parameters.\n",
        "\n",
        "Here's a step-by-step explanation of the code:\n",
        "1. **a, b = 0., 0.** - Set variables 'a' and 'b' to zero. Use 'a' to count total parameters and 'b' to count zero-valued parameters.\n",
        "2. **for p in model.parameters()** - Iterating over the parameters of the provided model, which are the learnable weights and biases of its layers.\n",
        "3. **a += p.numel()** - To add the number of elements in the parameter tensor 'p' to the variable 'a', use the numel() function.\n",
        "4. **b += (p == 0).sum() ** - To count the number of zeros in a tensor called \"p\", use \"p == 0\" to create a binary tensor and then use the \"sum()\" function to count the True values. Add the result to the variable \"b\".\n",
        "5. **return b / a** - Calculate the ratio of zero-valued parameters to the total number of parameters to determine the global sparsity of the model.\n",
        "\n",
        "Code obtained for the global sparity function:\n",
        "https://github.com/WongKinYiu/yolov7/blob/main/utils/torch_utils.py\n",
        "\n",
        "Martin, M. Local and global processing: The role of sparsity. Memory & Cognition 7, 476484 (1979). https://doi.org/10.3758/BF03198264"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpBC6dQ7UlUM"
      },
      "outputs": [],
      "source": [
        "# Code obtained from\n",
        "# https://github.com/WongKinYiu/yolov7/blob/main/utils/torch_utils.py\n",
        "def sparsity(model):\n",
        "    # Return global model sparsity\n",
        "    a, b = 0., 0.\n",
        "    for p in model.parameters():\n",
        "        a += p.numel()\n",
        "        b += (p == 0).sum()\n",
        "    return b / a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvTblBcyrGrt"
      },
      "source": [
        "### Local Pruning of the Trained Model\n",
        "\n",
        "This code below defines a local pruning function for a PyTorch neural network model. The function reduces the sparsity of the model by removing smaller connections.\n",
        "\n",
        "It calculates the threshold for pruning and generates a binary mask to indicate which weights should be pruned. The actual pruning is performed using PyTorch's `prune.custom_from_mask` method, which applies the generated mask to the weights of the linear layer.  This mask is applied to the weights of the linear layer and the pruned model is saved.\n",
        "\n",
        "Finally, the function prints the global sparsity of the pruned model and its new size.\n",
        "\n",
        "Reference:\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3d_Qf1PmfgX"
      },
      "outputs": [],
      "source": [
        "# defining the local pruning function\n",
        "def apply_local_prune(model,input_size, hidden_size1, hidden_size2,\n",
        "                output_size, pruning_amount):\n",
        "    # Create a new instance of the model\n",
        "    pruned_model = RegressionNN(input_size, hidden_size1,\n",
        "                                hidden_size2, output_size)\n",
        "\n",
        "    # Load the parameters from the original model\n",
        "    relevant_state_dict = {key: value for key, value in model.state_dict().items() if key in pruned_model.state_dict()}\n",
        "    pruned_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    for name, module in pruned_model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Calculate the threshold for pruning based on the pruning amount\n",
        "             # reference: https://stackoverflow.com/questions/61269052/remove-smallest-p-numbers-from-multiple-numpy-arrays\n",
        "            threshold = np.percentile(np.abs(module.weight.cpu().detach().numpy()),\n",
        "                                      100 * pruning_amount)\n",
        "\n",
        "            # Generate a custom mask based on the threshold\n",
        "            # Referemce: https://stackoverflow.com/questions/61629395/how-to-prune-weights-less-than-a-threshold-in-pytorch\n",
        "            custom_mask = torch.abs(module.weight) > threshold\n",
        "            custom_mask = custom_mask.to(torch.float32)\n",
        "\n",
        "            # Apply pruning using the custom mask\n",
        "            prune.custom_from_mask(module, name='weight', mask=custom_mask)\n",
        "\n",
        "            # Calculate sparsity for this layer\n",
        "            sparsity = torch.sum(custom_mask == 0).item() / custom_mask.numel()\n",
        "            print(f\"Sparsity in {name}: {sparsity:.2%}\")\n",
        "\n",
        "    # Save the pruned model with the pruning amount in the name\n",
        "    pruning_amount_int = int(pruning_amount * 100)\n",
        "    pruned_model_name = f'pruned_local_{pruning_amount_int}.pth'\n",
        "    torch.save(pruned_model.state_dict(), pruned_model_name)\n",
        "\n",
        "    return pruned_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yZv_ivkcJv"
      },
      "source": [
        "The function `compare_weights` compares weight matrices between an original model and a pruned model in the neural network. It iterates through the layers, ensuring that each linear layer's weight matrices match and prints a message if they don't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN1bPTEHkcJv"
      },
      "outputs": [],
      "source": [
        "# comparing the weights of the trained and pruned model\n",
        "def compare_weights(original_model, pruned_model):\n",
        "    print(\"Comparing weights between original and pruned models:\")\n",
        "\n",
        "    # Iterate through the layers of the models\n",
        "    for original_layer, pruned_layer in zip(original_model.children(), pruned_model.children()):\n",
        "        if isinstance(original_layer, nn.Linear):\n",
        "            original_weights = original_layer.weight.cpu().detach().numpy()\n",
        "            pruned_weights = pruned_layer.weight.cpu().detach().numpy()\n",
        "\n",
        "            # Check the shape of the weight matrices match\n",
        "            assert original_weights.shape == pruned_weights.shape, \"Shape mismatch in weight matrices\"\n",
        "\n",
        "            # Check the pruned weights are different from the original weights\n",
        "            if not np.array_equal(original_weights, pruned_weights):\n",
        "                layer_name = original_layer.__class__.__name__\n",
        "                print(f\"Weights in {layer_name} layer are different.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPgDZ7z7saSW"
      },
      "source": [
        "**Pruning Amount 0.6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQV4MnCOsaSW"
      },
      "outputs": [],
      "source": [
        "# Apply local pruning for pruning amount 0.5\n",
        "pruning_amount = 0.6\n",
        "pruned_local1 = apply_local_prune(model, input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size, pruning_amount)\n",
        "\n",
        "\n",
        "\n",
        "# Compare the weights of the loaded and pruned model\n",
        "compare_weights(model, pruned_local1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXXoKGkxsaSW"
      },
      "source": [
        "**Pruning Amount 0.7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eAiUqbCsaSW"
      },
      "outputs": [],
      "source": [
        "# Apply local pruning for pruning amount 0.7\n",
        "pruning_amount = 0.7\n",
        "pruned_local2 = apply_local_prune(model, input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size, pruning_amount)\n",
        "\n",
        "# Compare the weights of the loaded and pruned model\n",
        "compare_weights(model, pruned_local2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL9Qk_sjsaSX"
      },
      "source": [
        "**Pruning Amount 0.8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TsTxreysaSX"
      },
      "outputs": [],
      "source": [
        "# Apply local pruning for pruning amount 0.8\n",
        "pruning_amount = 0.8\n",
        "pruned_local3 = apply_local_prune(model, input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size, pruning_amount)\n",
        "\n",
        "# Compare the weights of the loaded and pruned model\n",
        "compare_weights(model, pruned_local3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7cgf7S4saSX"
      },
      "source": [
        "**Pruning Amount 0.9**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeLHXqUQsaSX"
      },
      "outputs": [],
      "source": [
        "# Apply local pruning for pruning amount 0.9\n",
        "pruning_amount = 0.9\n",
        "pruned_local4 = apply_local_prune(model, input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size, pruning_amount)\n",
        "\n",
        "# Compare the weights of the loaded and pruned model\n",
        "compare_weights(model, pruned_local4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zs37anX060D"
      },
      "outputs": [],
      "source": [
        "# Apply local pruning for pruning amount 0.8\n",
        "pruning_amount = 0.95\n",
        "pruned_local5 = apply_local_prune(model, input_size, hidden_size1,\n",
        "                                  hidden_size2, output_size, pruning_amount)\n",
        "\n",
        "# Compare the weights of the loaded and pruned model\n",
        "compare_weights(model, pruned_local5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB-A8Rc8KMsu"
      },
      "source": [
        "### Layer-Specific Local Pruning\n",
        "\n",
        "The code below defines a local pruning function that selectively prunes weights in fully-connected (linear) layers based on specified layer-wise sparsity values in the loaded model. The function prints a message indicating the start of the pruning process and iterates through each child module of the model, checking if the module is a linear layer. If a non-zero sparsity value is specified for the layer, the function proceeds with pruning. The function applies pruning to the linear layer using the prune.custom_from_mask method and prints the global sparsity of the pruned model and its new size.\n",
        "\n",
        "Reference used:\n",
        "\n",
        "https://towardsdatascience.com/how-to-prune-neural-networks-with-pytorch-ebef60316b91"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S30TbQwHECzI"
      },
      "outputs": [],
      "source": [
        "# function that removes characters that are not allowed in filenames\n",
        "# reference: https://docs.python.org/3/library/re.html\n",
        "def sanitize_for_filename(s):\n",
        "    return re.sub(r'[^\\w\\-_.]', '_', s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8CJdk-K-MJC"
      },
      "outputs": [],
      "source": [
        "# function for pruning per layer\n",
        "# reference: https://leimao.github.io/blog/PyTorch-Pruning/\n",
        "# defining the local pruning function with layer sparsity\n",
        "def local_prune_layer_sparsity(model, layer_sparsity_dict):\n",
        "    # Create a new instance of the model\n",
        "    pruned_model = RegressionNN(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "    # Load the parameters from the original model to the new model, filtering out irrelevant keys\n",
        "    relevant_state_dict = {key: value for key, value in model.state_dict().items() if key in pruned_model.state_dict()}\n",
        "    pruned_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    for name, module in pruned_model.named_children():\n",
        "        if isinstance(module, nn.Linear) and name in layer_sparsity_dict:\n",
        "            # Check if the layer has a specified sparsity in layer_sparsity_dict\n",
        "            layer_sparsity = layer_sparsity_dict[name]\n",
        "\n",
        "            if layer_sparsity > 0.0:\n",
        "                # Calculate the threshold for pruning based on the layer sparsity\n",
        "                # reference: https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor\n",
        "                threshold = np.percentile(np.abs(module.weight.cpu().detach().numpy()),\n",
        "                                          100 * layer_sparsity)\n",
        "\n",
        "                # Generate a custom mask based on the threshold\n",
        "                custom_mask = torch.abs(module.weight) > threshold\n",
        "                custom_mask = custom_mask.to(torch.float32)\n",
        "\n",
        "                # Apply pruning using the custom mask\n",
        "                prune.custom_from_mask(module, name='weight', mask=custom_mask)\n",
        "\n",
        "    # Transfer the pruned model's state to the original model\n",
        "    #model.load_state_dict(pruned_model.state_dict(), strict=False)\n",
        "\n",
        "    # Convert layer sparsity values to integers for better readability in filenames\n",
        "    layer_sparsity_int_dict = {k: int(v * 100) for k, v in layer_sparsity_dict.items()}\n",
        "\n",
        "    # Generate a filename-friendly representation of the layer sparsity dictionary\n",
        "    filename_friendly_repr = '_'.join(f'{sanitize_for_filename(k)}_{v}' for k, v in layer_sparsity_int_dict.items())\n",
        "\n",
        "    # Save the pruned model with the layer sparsity values in the name\n",
        "    pruned_model_name = f'pruned_local_layer_sparsity_{filename_friendly_repr}.pth'\n",
        "    torch.save(pruned_model.state_dict(), pruned_model_name)\n",
        "\n",
        "    return pruned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWVJeIldn9k"
      },
      "source": [
        "The `layer_sparsity_dict` allows you to set different levels of sparsity for each layer, which should add up to the desired overall sparsity of the model. The values for sparsity range from 0 (no pruning) to 1 (complete pruning).\n",
        "\n",
        "For instance, let's say for the three layers in our model called *fc1*, *fc2*, and *fc3*, and we want to reduce the overall size of the trained model by 70%. To achieve this, we have to assign sparsity values of 0.2, 0.3, and 0.2 respectively to each of the three layers. The sum of these values (0.2 + 0.3 + 0.2) equals 0.7, which achieves the desired overall sparsity of 70% for the entire model. We can adjust these values based on specific requirements for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a68fifdgWxe2"
      },
      "source": [
        "**Pruning Amount 0.6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pItSRYlZWxe2"
      },
      "outputs": [],
      "source": [
        "# Specify layer-wise sparsity as a dictionary - resulting pruning amount 0.6\n",
        "layer_sparsity_dict_06 = {'fc1': 0.2, 'fc2': 0.2, 'fc3': 0.2}\n",
        "\n",
        "# Apply local pruning\n",
        "pruned_sparse1 = local_prune_layer_sparsity(model, layer_sparsity_dict_06)\n",
        "\n",
        "#compare weights of the pruned and loaded model\n",
        "compare_weights(model, pruned_sparse1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTqlYyTBWxe2"
      },
      "source": [
        "**Pruning Amount 0.7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DffRwm6Wxe2"
      },
      "outputs": [],
      "source": [
        "# Specify layer-wise sparsity as a dictionary - resulting pruning amount 0.7\n",
        "layer_sparsity_dict_07 = {'fc1': 0.2, 'fc2': 0.3, 'fc3': 0.2}\n",
        "\n",
        "# Apply local pruning\n",
        "pruned_sparse2 = local_prune_layer_sparsity(model, layer_sparsity_dict_07)\n",
        "\n",
        "#compare weights of the pruned and loaded model\n",
        "compare_weights(model, pruned_sparse2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FfK69rcWxe2"
      },
      "source": [
        "**Pruning Amount 0.8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62B-k9EUWxe3"
      },
      "outputs": [],
      "source": [
        "# Specify layer-wise sparsity as a dictionary - resulting pruning amount 0.8\n",
        "layer_sparsity_dict_08 = {'fc1': 0.25, 'fc2': 0.25, 'fc3': 0.3}\n",
        "\n",
        "# Apply local pruning\n",
        "pruned_sparse3 = local_prune_layer_sparsity(model, layer_sparsity_dict_08)\n",
        "\n",
        "#compare weights of the pruned and loaded model\n",
        "compare_weights(model, pruned_sparse3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JQp3ssmWxe3"
      },
      "source": [
        "**Pruning Amount 0.9**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHi_F_I3Wxe3"
      },
      "outputs": [],
      "source": [
        "# Specify layer-wise sparsity as a dictionary - resulting pruning amount 0.9\n",
        "layer_sparsity_dict_09 = {'fc1': 0.3, 'fc2': 0.3, 'fc3': 0.3}\n",
        "\n",
        "# Apply local pruning\n",
        "pruned_sparse4 = local_prune_layer_sparsity(model, layer_sparsity_dict_09)\n",
        "\n",
        "#compare weights of the pruned and loaded model\n",
        "compare_weights(model, pruned_sparse4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHD0-_q9fwEC"
      },
      "outputs": [],
      "source": [
        "# Specify layer-wise sparsity as a dictionary - resulting pruning amount 0.95\n",
        "layer_sparsity_dict_095 = {'fc1': 0.31, 'fc2': 0.31, 'fc3': 0.33}\n",
        "\n",
        "# Apply local pruning\n",
        "pruned_sparse5 = local_prune_layer_sparsity(model, layer_sparsity_dict_095)\n",
        "\n",
        "#compare weights of the pruned and loaded model\n",
        "compare_weights(model, pruned_sparse5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiLD46CF9JrK"
      },
      "source": [
        "## Principal Component Analysis (PCA) on a Pruned Model\n",
        "\n",
        "In this section, our goal is to reduce the number of input features on the first layer of the locally pruned model, fc1, by applying PCA compression. To achieve this, we will use a typical approach that involves applying PCA in a way that compresses the dimensionality of the input space while preserving the essence of the transformation that the original weights provided. This approach modifies the architecture of the neural network differently from reducing the number of input features across the layers.\n",
        "\n",
        "Reference:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMaSs9E3jRUr"
      },
      "outputs": [],
      "source": [
        "# Define the apply_pca_and_reshape function\n",
        "def apply_pca_and_reshape(weights, n_components):\n",
        "  # apply PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    # reducing the output featurures and transforming them\n",
        "    transformed_weights = pca.fit_transform(weights.T).T\n",
        "    return transformed_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2adpIjplmks"
      },
      "outputs": [],
      "source": [
        "# Modifying the RegressionNN to accommodate the reduced dimensions\n",
        "class CompressedRegressionNN(nn.Module):\n",
        "    def __init__(self, input_size, pca_fc1_out_features,\n",
        "                 pca_fc2_out_features, output_size):\n",
        "        super(CompressedRegressionNN, self).__init__()\n",
        "\n",
        "        # Adjusted first layer with reduced output features\n",
        "        self.fc1 = nn.Linear(input_size, pca_fc1_out_features)\n",
        "\n",
        "        # Adjusted second layer with reduced output features\n",
        "        self.fc2 = nn.Linear(pca_fc1_out_features, pca_fc2_out_features)\n",
        "\n",
        "        # Third layer remains unchanged\n",
        "        self.fc3 = nn.Linear(pca_fc2_out_features, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpvg6kjtCe4z"
      },
      "outputs": [],
      "source": [
        "# Original models hidden_size1, hidden_size2, input and output sizes\n",
        "input_size = X_train.shape[1] # input features = 58\n",
        "output_size = 1  # Output features for the third layer\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoTtkDxQcZaf"
      },
      "source": [
        "### Functions to Train and Validate Compressed Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v2RMoO7lBGQ"
      },
      "source": [
        "Testing the compressed model without training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fj1VwzrlBGQ"
      },
      "outputs": [],
      "source": [
        "# function to evaluate the model without training\n",
        "# model - model to evaluate\n",
        "# test_loader - DataLoader for the test dataset\n",
        "# device (torch.device) - device to run the evaluation on ('cuda' or 'cpu')\n",
        "def evaluate_model(model, test_loader, device):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    criterion = nn.MSELoss()  # Initialize the loss function\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_mae = 0.0\n",
        "    samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculations needed\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)  # Make predictions\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "\n",
        "            mae = torch.mean(torch.abs(outputs - targets)).item()\n",
        "            total_mae += mae * inputs.size(0)  # Accumulate MAE\n",
        "\n",
        "            samples += inputs.size(0)  # Keep track of total samples\n",
        "\n",
        "    average_loss = total_loss / samples\n",
        "    average_mae = total_mae / samples\n",
        "\n",
        "    return average_loss, average_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_jAe964lE-b"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "average_loss, average_mae = evaluate_model(PCA_local_05, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YvX3RW2lMXH"
      },
      "source": [
        "Training and Validating the PCA compressed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpoW4WbRlMXH"
      },
      "outputs": [],
      "source": [
        "# Function to train and validate the model\n",
        "# Reference: https://discuss.pytorch.org/t/output-evaluation-loss-after-every-n-batches-instead-of-epochs-with-pytorch/116619\n",
        "def train_and_validate(model, train_loader, test_loader, learning_rate,\n",
        "                       epochs, max_grad_norm=None):\n",
        "\n",
        "    # Move the model to CUDA if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining Mean Squared Error loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Lists to store losses\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_mae = 0.0\n",
        "\n",
        "        # Training\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            mse_loss = criterion(outputs.view(-1), targets.view(-1))\n",
        "            mae_loss = nn.L1Loss()(outputs.view(-1), targets.view(-1))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            mse_loss.backward()\n",
        "\n",
        "            # Gradient Clipping\n",
        "            if max_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += mse_loss.item()\n",
        "            total_mae += mae_loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        average_mae = total_mae / len(train_loader)\n",
        "        training_losses.append(average_loss)\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], MSE Loss (Train): {average_loss:.3f}, MAE Loss (Train): {average_mae:.3f}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        validation_loss, validation_mae = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "                outputs = model(inputs)\n",
        "                validation_loss += criterion(outputs.view(-1), targets.view(-1)).item()\n",
        "                validation_mae += nn.L1Loss()(outputs.view(-1), targets.view(-1)).item()\n",
        "\n",
        "        average_validation_loss = validation_loss / len(test_loader)\n",
        "        average_validation_mae = validation_mae / len(test_loader)\n",
        "        validation_losses.append(average_validation_loss)\n",
        "        print(f'Validation Loss: {average_validation_loss:.3f}, Validation MAE: {average_validation_mae:.3f}')\n",
        "\n",
        "    # Plotting losses\n",
        "    plt.plot(range(1, epochs + 1), training_losses, label='Training Loss')\n",
        "    plt.plot(range(1, epochs + 1), validation_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model, training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B1RYSlBlN_m"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "    model=PCA_local_05, train_loader=train_loader,\n",
        "    test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3xjvtwYxDBd"
      },
      "source": [
        "This function transfers weights (and biases) from a pruned model to another. It takes the source model (pruned model) and the target model as arguments. It applies existing masks to the weights, computes pruned weights, and updates corresponding layers in the target model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54zcz2O5xDB8"
      },
      "outputs": [],
      "source": [
        "# Copying pruned weights (and biases) from a source model to a target model\n",
        "# source_model (torch.nn.Module): The model from which to transfer weights.\n",
        "# target_model (torch.nn.Module): The model to which weights will be transferred.\n",
        "# reference: https://jimmy-shen.medium.com/pytorch-freeze-part-of-the-layers-4554105e03a6\n",
        "def transfer_pruned_weights(source_model, target_model):\n",
        "    for name, module in source_model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Retrieve the original weight and mask\n",
        "            orig_weight = module.weight_orig if hasattr(module, 'weight_orig') else module.weight\n",
        "            mask = module.weight_mask if hasattr(module, 'weight_mask') else torch.ones_like(module.weight)\n",
        "\n",
        "            # Apply the mask to get the pruned weight\n",
        "            # https://stackoverflow.com/questions/69311857/why-doesnt-torch-pruning-actually-remove-filters-or-weights\n",
        "            pruned_weight = orig_weight * mask\n",
        "\n",
        "            # Manually update the weights in the target model\n",
        "            target_layer = getattr(target_model, name)\n",
        "            target_layer.weight = nn.Parameter(pruned_weight)\n",
        "\n",
        "            # Transfer the bias if it exists\n",
        "            if module.bias is not None:\n",
        "              # https://stackoverflow.com/questions/73228490/how-to-set-specific-values-for-the-weight-and-bias-in-a-neural-net\n",
        "                target_layer.bias = nn.Parameter(module.bias.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TKf_KNAliHt"
      },
      "source": [
        "**Checking the weight shape of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H8aJGholiH9"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded model\n",
        "print(\"fc1 weight shape:\", model.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", model.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", model.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG2CCmVCPj5Q"
      },
      "source": [
        "### PCA on Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2veCg5VnI617"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_local1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs10MbXwLwl3"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecaV9BC4gHGM"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an99RWZtgHGN"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fYWeJE8gHGO"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights = temp_l1_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_5030 = apply_pca_and_reshape(original_fc1_weights, 50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights = temp_l1_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_5030 = apply_pca_and_reshape(original_fc2_weights, 30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_5030).float().view(50, 58)\n",
        "PCA_local1_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_5030.fc1.bias.data.fill_(0)\n",
        "PCA_local1_5030.fc2.bias.data.fill_(0)\n",
        "PCA_local1_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSw3KFuoLwmC"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htQJ8-_kQaS2"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDDrInelLwmC"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9jTgrGPiD5w"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ovLWqyliD58"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDE8bgTJiD59"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l1_5010 = temp_l1_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_5010 = apply_pca_and_reshape(original_fc1_weights_l1_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_5010 = temp_l1_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_5010 = apply_pca_and_reshape(original_fc2_weights_l1_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_5010).float().view(50, 58)\n",
        "PCA_local1_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_5010.fc1.bias.data.fill_(0)\n",
        "PCA_local1_5010.fc2.bias.data.fill_(0)\n",
        "PCA_local1_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdY4Vu9-iD59"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1B7dHImiD59"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fEKrZDULwmD"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDfI5epeiMi6"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKsnjMpViMjH"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDGXAekjiMjH"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l1_505 = temp_l1_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_505 = apply_pca_and_reshape(original_fc1_weights_l1_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_505 = temp_l1_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_505 = apply_pca_and_reshape(original_fc2_weights_l1_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_505).float().view(50, 58)\n",
        "PCA_local1_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_505.fc1.bias.data.fill_(0)\n",
        "PCA_local1_505.fc2.bias.data.fill_(0)\n",
        "PCA_local1_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypcXPmORiMjI"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq3FcMnLiMjI"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY1n611ih30g"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfeVEbBhiNLC"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GWFKUSniNLC"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAHbzY0IiNLD"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l1_3010 = temp_l1_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_3010 = apply_pca_and_reshape(original_fc1_weights_l1_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_3010 = temp_l1_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_3010 = apply_pca_and_reshape(original_fc2_weights_l1_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_3010).float().view(30, 58)\n",
        "PCA_local1_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_3010.fc1.bias.data.fill_(0)\n",
        "PCA_local1_3010.fc2.bias.data.fill_(0)\n",
        "PCA_local1_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QQi8wLOiNLD"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5oRBxBFiNLD"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13X15BVJh6EQ"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv2EmBybiN1J"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRkaBEH8iN1J"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWPMD1ldiN1J"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l1_305 = temp_l1_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_305 = apply_pca_and_reshape(original_fc1_weights_l1_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_305 = temp_l1_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_305 = apply_pca_and_reshape(original_fc2_weights_l1_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_305).float().view(30, 58)\n",
        "PCA_local1_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_305.fc1.bias.data.fill_(0)\n",
        "PCA_local1_305.fc2.bias.data.fill_(0)\n",
        "PCA_local1_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhJ0mxtTiN1K"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1OG_abkiN1K"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZhZqenJh8V4"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USGNqJ7oiYn1"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l1_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, temp_l1_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffXIAHIRiYn2"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFsMDcxuiYn2"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l1_105 = temp_l1_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l1_105 = apply_pca_and_reshape(original_fc1_weights_l1_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_105 = temp_l1_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l1_105 = apply_pca_and_reshape(original_fc2_weights_l1_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local1_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local1_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l1_105).float().view(10, 58)\n",
        "PCA_local1_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l1_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local1_105.fc1.bias.data.fill_(0)\n",
        "PCA_local1_105.fc2.bias.data.fill_(0)\n",
        "PCA_local1_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local1_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local1_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local1_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local1_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhmrtditiYn3"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local1_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsTDaDcUiYn3"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local1_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1UkFuTxJAMy"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_local2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu4jNIE1pX89"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvSdARQ8pX9L"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUFYNhoupX9M"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a63HBnQpX9M"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_5030 = temp_l2_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_5030 = apply_pca_and_reshape(original_fc1_weights_l2_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l2_5030 = temp_l2_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_5030 = apply_pca_and_reshape(original_fc2_weights_l2_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_5030).float().view(50, 58)\n",
        "PCA_local2_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_5030.fc1.bias.data.fill_(0)\n",
        "PCA_local2_5030.fc2.bias.data.fill_(0)\n",
        "PCA_local2_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCuk3c9VpX9N"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE9nWrxxpX9N"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpVDexbvpX9N"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjlHYtEfpX9N"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMVEfLuspX9O"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_8d3NV9pX9O"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_5010 = temp_l2_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_5010 = apply_pca_and_reshape(original_fc1_weights_l2_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l2_5010 = temp_l2_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_5010 = apply_pca_and_reshape(original_fc2_weights_l2_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_5010).float().view(50, 58)\n",
        "PCA_local2_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_5010.fc1.bias.data.fill_(0)\n",
        "PCA_local2_5010.fc2.bias.data.fill_(0)\n",
        "PCA_local2_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNsHQOD2pX9O"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a67r-gkgpX9O"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZEGtUfxpX9O"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-nviGDApX9P"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWlqajuqpX9P"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwFLvBa3pX9P"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_505 = temp_l2_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_505 = apply_pca_and_reshape(original_fc1_weights_l2_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l2_505 = temp_l2_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_505 = apply_pca_and_reshape(original_fc2_weights_l2_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_505).float().view(50, 58)\n",
        "PCA_local2_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_505.fc1.bias.data.fill_(0)\n",
        "PCA_local2_505.fc2.bias.data.fill_(0)\n",
        "PCA_local2_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSFRejg6pX9P"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_-xII6MpX9P"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFLlVgQIpX9P"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl7AYUz1pX9Q"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKsKz1j2pX9Q"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0-Mo4X1pX9Q"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_3010 = temp_l2_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_3010 = apply_pca_and_reshape(original_fc1_weights_l2_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l2_3010 = temp_l2_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_3010 = apply_pca_and_reshape(original_fc2_weights_l2_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_3010).float().view(30, 58)\n",
        "PCA_local2_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_3010.fc1.bias.data.fill_(0)\n",
        "PCA_local2_3010.fc2.bias.data.fill_(0)\n",
        "PCA_local2_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56MTHOpzpX9Q"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IvVyGjipX9Q"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnlB-KUpX9R"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa4G1s49pX9R"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc55ALdOpX9R"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEroqSqNpX9R"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_305 = temp_l2_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_305 = apply_pca_and_reshape(original_fc1_weights_l2_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l2_305 = temp_l2_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_305 = apply_pca_and_reshape(original_fc2_weights_l2_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_305).float().view(30, 58)\n",
        "PCA_local2_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_305.fc1.bias.data.fill_(0)\n",
        "PCA_local2_305.fc2.bias.data.fill_(0)\n",
        "PCA_local2_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kdOA1eppX9S"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kkm7O96pX9S"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTF0jNMnpX9S"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVPG7IPLpX9S"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l2_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, temp_l2_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CefjJIOvpX9S"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l2_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l2_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l2_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFje2K3KpX9T"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l2_105 = temp_l2_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_105 = apply_pca_and_reshape(original_fc1_weights_l2_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l1_105 = temp_l2_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_105 = apply_pca_and_reshape(original_fc2_weights_l1_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local2_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local2_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_105).float().view(10, 58)\n",
        "PCA_local2_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local2_105.fc1.bias.data.fill_(0)\n",
        "PCA_local2_105.fc2.bias.data.fill_(0)\n",
        "PCA_local2_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local2_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local2_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local2_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local2_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StG5LXTRpX9T"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local2_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp2--raGpX9T"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local2_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeHKOJT-JDJ8"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_local3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqGzxsjkpZFn"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj-HOllApZFo"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymNR7q3ZpZFo"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pire2HokpZFo"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_5030 = temp_l2_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l2_5030 = apply_pca_and_reshape(original_fc1_weights_l3_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l2_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_5030 = temp_l2_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l2_5030 = apply_pca_and_reshape(original_fc2_weights_l3_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l2_5030).float().view(50, 58)\n",
        "PCA_local3_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l2_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_5030.fc1.bias.data.fill_(0)\n",
        "PCA_local3_5030.fc2.bias.data.fill_(0)\n",
        "PCA_local3_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mU--TrqpZFq"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grliv4ItpZFr"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivj3wa7-pZFr"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TeBMlTSpZFs"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RCfrFqvpZFs"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyLy63CTpZFs"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_5010 = temp_l3_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l3_5010 = apply_pca_and_reshape(original_fc1_weights_l3_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l3_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_5010 = temp_l3_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l3_5010 = apply_pca_and_reshape(original_fc2_weights_l3_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l3_5010).float().view(50, 58)\n",
        "PCA_local3_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l3_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_5010.fc1.bias.data.fill_(0)\n",
        "PCA_local3_5010.fc2.bias.data.fill_(0)\n",
        "PCA_local3_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBV74JPSpZFs"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTtw5gaopZFs"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyi2OB61pZFt"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deceH7YQpZFt"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6tagyc1pZFt"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXTraH8QpZFt"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_505 = temp_l3_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l3_505 = apply_pca_and_reshape(original_fc1_weights_l3_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l3_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_505 = temp_l3_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l3_505 = apply_pca_and_reshape(original_fc2_weights_l3_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l3_505).float().view(50, 58)\n",
        "PCA_local3_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l3_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_505.fc1.bias.data.fill_(0)\n",
        "PCA_local3_505.fc2.bias.data.fill_(0)\n",
        "PCA_local3_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5_TU4JZpZFt"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6SnXb6OpZFu"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSiOyS09pZFu"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az5HNL57pZFu"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "657a3EiZpZFu"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbIKc1DPpZFu"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_3010 = temp_l3_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l3_3010 = apply_pca_and_reshape(original_fc1_weights_l3_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l3_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_3010 = temp_l3_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l3_3010 = apply_pca_and_reshape(original_fc2_weights_l3_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l3_3010).float().view(30, 58)\n",
        "PCA_local3_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l3_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_3010.fc1.bias.data.fill_(0)\n",
        "PCA_local3_3010.fc2.bias.data.fill_(0)\n",
        "PCA_local3_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJjCOqmvpZFu"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf10jlODpZFv"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5330Bz7pZFv"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJXJurS8pZFv"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUKYjxJIpZFv"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iXSFoP2pZFv"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_305 = temp_l3_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l3_305 = apply_pca_and_reshape(original_fc1_weights_l3_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l3_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_305 = temp_l3_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l3_305 = apply_pca_and_reshape(original_fc2_weights_l3_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l3_305).float().view(30, 58)\n",
        "PCA_local3_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l3_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_305.fc1.bias.data.fill_(0)\n",
        "PCA_local3_305.fc2.bias.data.fill_(0)\n",
        "PCA_local3_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_eEKr9IpZFv"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdUPNOR9pZFw"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd-KGLlIpZFw"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvNN83ijpZFw"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l3_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, temp_l3_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilfcBf1rpZFw"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l3_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l3_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l3_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_sQvYGMpZFw"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l3_105 = temp_l3_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l3_105 = apply_pca_and_reshape(original_fc1_weights_l3_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l3_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l3_105 = temp_l3_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l3_105 = apply_pca_and_reshape(original_fc2_weights_l3_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local3_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local3_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l3_105).float().view(10, 58)\n",
        "PCA_local3_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l3_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local3_105.fc1.bias.data.fill_(0)\n",
        "PCA_local3_105.fc2.bias.data.fill_(0)\n",
        "PCA_local3_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local3_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local3_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local3_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local3_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoLnj_y3pZFx"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local3_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZauqCsdpZFx"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local3_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T-yYYoOJFzf"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_local4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu2YN8mXpaFO"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-kloslqpaFP"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Js8AILypaFP"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZIQ6SjrpaFP"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_5030 = temp_l4_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_5030 = apply_pca_and_reshape(original_fc1_weights_l4_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_5030 = temp_l4_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_5030 = apply_pca_and_reshape(original_fc2_weights_l4_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_5030).float().view(50, 58)\n",
        "PCA_local4_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_5030.fc1.bias.data.fill_(0)\n",
        "PCA_local4_5030.fc2.bias.data.fill_(0)\n",
        "PCA_local4_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naUFwYMUpaFP"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95L3WZl8paFQ"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfrgE6zpaFQ"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9N9ZeHIpaFQ"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5AiIG0npaFQ"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZcJTETTpaFR"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_5010 = temp_l4_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_5010 = apply_pca_and_reshape(original_fc1_weights_l4_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_5010 = temp_l4_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_5010 = apply_pca_and_reshape(original_fc2_weights_l4_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_5010).float().view(50, 58)\n",
        "PCA_local4_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_5010.fc1.bias.data.fill_(0)\n",
        "PCA_local4_5010.fc2.bias.data.fill_(0)\n",
        "PCA_local4_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jInHClHNpaFR"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrEPNL5GpaFR"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yVUKem-paFR"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBqF_ygapaFR"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We_ooavApaFR"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2lL2_iwpaFS"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_505 = temp_l4_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_505 = apply_pca_and_reshape(original_fc1_weights_l4_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_505 = temp_l4_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_505 = apply_pca_and_reshape(original_fc2_weights_l4_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_505).float().view(50, 58)\n",
        "PCA_local4_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_505.fc1.bias.data.fill_(0)\n",
        "PCA_local4_505.fc2.bias.data.fill_(0)\n",
        "PCA_local4_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_fAVB0epaFS"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9S3B1EnpaFS"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyczdCyjpaFS"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94UvF3UdpaFS"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlmfHnrxpaFS"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00NeCxswpaFS"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_3010 = temp_l4_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_3010 = apply_pca_and_reshape(original_fc1_weights_l4_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_3010 = temp_l4_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_3010 = apply_pca_and_reshape(original_fc2_weights_l4_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_3010).float().view(30, 58)\n",
        "PCA_local4_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_3010.fc1.bias.data.fill_(0)\n",
        "PCA_local4_3010.fc2.bias.data.fill_(0)\n",
        "PCA_local4_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bElE4JHlpaFT"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2XtkB8lpaFT"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvafMTrUpaFT"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMQhkUsOpaFT"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hYzvn9YpaFT"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Tf5KDrpaFT"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_305 = temp_l4_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_305 = apply_pca_and_reshape(original_fc1_weights_l4_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_305 = temp_l4_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_305 = apply_pca_and_reshape(original_fc2_weights_l4_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_305).float().view(30, 58)\n",
        "PCA_local4_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_305.fc1.bias.data.fill_(0)\n",
        "PCA_local4_305.fc2.bias.data.fill_(0)\n",
        "PCA_local4_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZQ5u02epaFU"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqDt1AYupaFU"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GUQWwuvpaFU"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED1uEAFVpaFU"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l4_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, temp_l4_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuv6EfG6paFU"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l4_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l4_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l4_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkK5eIaPpaFU"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l4_105 = temp_l4_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l4_105 = apply_pca_and_reshape(original_fc1_weights_l4_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l4_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l4_105 = temp_l4_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l4_105 = apply_pca_and_reshape(original_fc2_weights_l4_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local4_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local4_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l4_105).float().view(10, 58)\n",
        "PCA_local4_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l4_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local4_105.fc1.bias.data.fill_(0)\n",
        "PCA_local4_105.fc2.bias.data.fill_(0)\n",
        "PCA_local4_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local4_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local4_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local4_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local4_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzvjMB_kpaFV"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local4_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrULbQrQpaFV"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local4_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-QxenTfJIhk"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_local5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aOzHj1tpben"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLg5EYJ7pben"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td96es6Opbeo"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN31pCsipbeo"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_5030 = temp_l5_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_5030 = apply_pca_and_reshape(original_fc1_weights_l5_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_5030 = temp_l5_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_5030 = apply_pca_and_reshape(original_fc2_weights_l5_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_5030).float().view(50, 58)\n",
        "PCA_local5_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_5030.fc1.bias.data.fill_(0)\n",
        "PCA_local5_5030.fc2.bias.data.fill_(0)\n",
        "PCA_local5_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqE65cJ0pbep"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-DdRox3pbep"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGDYV0kYpbep"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYUiY_DKpbep"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm-tp5Yepbeq"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8K6x0JApbeq"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_5010 = temp_l5_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_5010 = apply_pca_and_reshape(original_fc1_weights_l5_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_5010 = temp_l5_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_5010 = apply_pca_and_reshape(original_fc2_weights_l5_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_5010).float().view(50, 58)\n",
        "PCA_local5_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_5010.fc1.bias.data.fill_(0)\n",
        "PCA_local5_5010.fc2.bias.data.fill_(0)\n",
        "PCA_local5_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojqsMrLDpbeq"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqHhX7atpbeq"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCJj9L3Jpbeq"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vl9TBBOpber"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X68Di96Kpber"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWsOmB55pber"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_505 = temp_l5_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_505 = apply_pca_and_reshape(original_fc1_weights_l5_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_505 = temp_l5_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_505 = apply_pca_and_reshape(original_fc2_weights_l5_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_505).float().view(50, 58)\n",
        "PCA_local5_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_505.fc1.bias.data.fill_(0)\n",
        "PCA_local5_505.fc2.bias.data.fill_(0)\n",
        "PCA_local5_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXVCXOL_pber"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmANg2Ympber"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrI3V-B2pber"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es2JDfX_pbes"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMg1ipWvpbes"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PePcMYcOpbes"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_3010 = temp_l5_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_3010 = apply_pca_and_reshape(original_fc1_weights_l5_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_3010 = temp_l5_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_3010 = apply_pca_and_reshape(original_fc2_weights_l5_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_3010).float().view(30, 58)\n",
        "PCA_local5_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_3010.fc1.bias.data.fill_(0)\n",
        "PCA_local5_3010.fc2.bias.data.fill_(0)\n",
        "PCA_local5_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPVdQYxipbes"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fL-ZTITpbes"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEOBFKftpbes"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joZ1I3_Epbes"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQU_W-jDpbet"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOB52hvVpbet"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_305 = temp_l5_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_305 = apply_pca_and_reshape(original_fc1_weights_l5_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_305 = temp_l5_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_305 = apply_pca_and_reshape(original_fc2_weights_l5_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_305).float().view(30, 58)\n",
        "PCA_local5_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_305.fc1.bias.data.fill_(0)\n",
        "PCA_local5_305.fc2.bias.data.fill_(0)\n",
        "PCA_local5_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxTcO2Z4pbet"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjTZLE7Lpbet"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P2acYj8pbet"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5KcFO7ipbet"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_l5_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, temp_l5_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KFP0QmXpbeu"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l5_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l5_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l5_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue2bzCr2pbeu"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_l5_105 = temp_l5_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_l5_105 = apply_pca_and_reshape(original_fc1_weights_l5_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l5_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_l5_105 = temp_l5_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_l5_105 = apply_pca_and_reshape(original_fc2_weights_l5_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_local5_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_local5_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_l5_105).float().view(10, 58)\n",
        "PCA_local5_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_l5_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_local5_105.fc1.bias.data.fill_(0)\n",
        "PCA_local5_105.fc2.bias.data.fill_(0)\n",
        "PCA_local5_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_local5_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_local5_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_local5_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_local5_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRiJQIgepbev"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_local5_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmr7FuSUpbev"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_local5_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h5fI9rlKMlF"
      },
      "source": [
        "### PCA on Layer-Wise Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQXQyMoaKeB2"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_sparse1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx8tYuG_piRi"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEj9y7rNpiRj"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1BSU67upiRj"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl1_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl1_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl1_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjn1hOkipiRj"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_5030 = temp_sl1_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_5030 = apply_pca_and_reshape(original_fc1_weights_sl1_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl1_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_5030 = temp_sl1_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_5030 = apply_pca_and_reshape(original_fc2_weights_sl1_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_5030).float().view(50, 58)\n",
        "PCA_sparse1_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_5030.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_5030.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OZvLjY0piRj"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTyXB0dbpiRk"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3afjXe_piRk"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFNfec-KpiRk"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QphybBEfpiRk"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl1_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl1_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl1_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOQ1HGQLpiRk"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_5010 = temp_sl1_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_5010 = apply_pca_and_reshape(original_fc1_weights_sl1_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl1_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_5010 = temp_sl1_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_5010 = apply_pca_and_reshape(original_fc2_weights_sl1_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_5010).float().view(50, 58)\n",
        "PCA_sparse1_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_5010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_5010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ5ZXP4gpiRl"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-12abhCHpiRl"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGl1VEg4piRl"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYXsvkSApiRl"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiFbnGKSpiRl"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl1_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl1_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl1_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M4IIvyopiRl"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_505 = temp_sl1_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_505 = apply_pca_and_reshape(original_fc1_weights_sl1_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl1_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_505 = temp_sl1_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_505 = apply_pca_and_reshape(original_fc2_weights_sl1_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_505).float().view(50, 58)\n",
        "PCA_sparse1_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_505.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_505.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x08G-Mv3piRm"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_CHizHcpiRm"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6vm9oUqpiRm"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KLda6A9piRm"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v4nQ3tqpiRm"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_l1_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_l1_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_l1_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nucgvrTPpiRm"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_3010 = temp_l1_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_3010 = apply_pca_and_reshape(original_fc1_weights_sl1_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_l1_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_3010 = temp_l1_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_3010 = apply_pca_and_reshape(original_fc2_weights_sl1_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_3010).float().view(30, 58)\n",
        "PCA_sparse1_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_3010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_3010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1AbQlm_piRn"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEcM1zRTpiRn"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuD1sID4piRn"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLpeUZ39piRn"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBvge1m4piRn"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl1_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl1_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl1_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbVtmsz2piRn"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_305 = temp_sl1_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_305 = apply_pca_and_reshape(original_fc1_weights_sl1_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl1_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_305 = temp_sl1_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_305 = apply_pca_and_reshape(original_fc2_weights_sl1_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_305).float().view(30, 58)\n",
        "PCA_sparse1_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_305.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_305.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2zvewJypiRo"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y48Qiy0piRo"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDBXUizPpiRo"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_LiN6TWpiRo"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl1_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, temp_sl1_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df-IIdM-piRo"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl1_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl1_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl1_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exjGMBpRpiRp"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl1_105 = temp_sl1_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl1_105 = apply_pca_and_reshape(original_fc1_weights_sl1_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl1_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl1_105 = temp_sl1_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl1_105 = apply_pca_and_reshape(original_fc2_weights_sl1_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse1_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse1_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl1_105).float().view(10, 58)\n",
        "PCA_sparse1_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl1_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse1_105.fc1.bias.data.fill_(0)\n",
        "PCA_sparse1_105.fc2.bias.data.fill_(0)\n",
        "PCA_sparse1_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse1_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse1_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse1_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse1_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUDSqVNSpiRp"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse1_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hreKYu_1piRp"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse1_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agSbt_cEKrFI"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_sparse2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDnJpzc10smP"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8-OY0-q0sma"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqnzM2SG0sma"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9kfXQSq0sma"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_5030 = temp_sl2_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_5030 = apply_pca_and_reshape(original_fc1_weights_sl2_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_5030 = temp_sl2_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_5030 = apply_pca_and_reshape(original_fc2_weights_sl2_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_5030).float().view(50, 58)\n",
        "PCA_sparse2_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_5030.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_5030.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RqwaI8H0sma"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKunPDwy0sma"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTEeuTYF0sma"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKW_nV1t0smb"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLUIMBRh0smb"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1pidS7l0smb"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_5010 = temp_sl2_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_5010 = apply_pca_and_reshape(original_fc1_weights_sl2_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_5010 = temp_sl2_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_5010 = apply_pca_and_reshape(original_fc2_weights_sl2_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_5010).float().view(50, 58)\n",
        "PCA_sparse2_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_5010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_5010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIqyTqOv0smb"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StFoT4yt0smb"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e_ZG2dm0smb"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIK4SBlM0smb"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVvdNEUp0smc"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw_FPAwU0smc"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_505 = temp_sl2_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_505 = apply_pca_and_reshape(original_fc1_weights_sl2_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_505 = temp_sl2_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_505 = apply_pca_and_reshape(original_fc2_weights_sl2_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_505).float().view(50, 58)\n",
        "PCA_sparse2_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_505.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_505.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLoq6jlb0smc"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bFoLWWj0smc"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGOqA5WE0smc"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WkoFHS50smc"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyJpEbY-0smc"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUlV2iSY0smd"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_3010 = temp_sl2_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_3010 = apply_pca_and_reshape(original_fc1_weights_sl2_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_3010 = temp_sl2_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_3010 = apply_pca_and_reshape(original_fc2_weights_sl2_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_3010).float().view(30, 58)\n",
        "PCA_sparse2_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_3010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_3010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDZMjuGk0smd"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gpqCD5H0sme"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SEB4jp_0sme"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTB7STf90smf"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxEjsRwj0smf"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzsK6VPl0smf"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_305 = temp_sl2_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_305 = apply_pca_and_reshape(original_fc1_weights_sl2_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_305 = temp_sl2_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_305 = apply_pca_and_reshape(original_fc2_weights_sl2_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_305).float().view(30, 58)\n",
        "PCA_sparse2_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_305.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_305.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cDyy_-K0smg"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_vnT65K0smg"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjIRHCJ90smg"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfThPXws0smg"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl2_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, temp_sl2_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyC89qLs0smg"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl2_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl2_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl2_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcaSrPJv0smg"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl2_105 = temp_sl2_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl2_105 = apply_pca_and_reshape(original_fc1_weights_sl2_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl2_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl2_105 = temp_sl2_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl2_105 = apply_pca_and_reshape(original_fc2_weights_sl2_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse2_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse2_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl2_105).float().view(10, 58)\n",
        "PCA_sparse2_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl2_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse2_105.fc1.bias.data.fill_(0)\n",
        "PCA_sparse2_105.fc2.bias.data.fill_(0)\n",
        "PCA_sparse2_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse2_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse2_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse2_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse2_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9keQ3KPo0smg"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse2_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tFWUqNv0smg"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse2_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2f_xufMKvh4"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_sparse3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k5K_Nvx2hli"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNpZ0-m12hlu"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m_VqAPw2hlu"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq04wNBG2hlv"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_5030 = temp_sl3_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_5030 = apply_pca_and_reshape(original_fc1_weights_sl3_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_5030 = temp_sl3_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_5030 = apply_pca_and_reshape(original_fc2_weights_sl3_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_5030).float().view(50, 58)\n",
        "PCA_sparse3_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_5030.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_5030.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr1G19sM2hlv"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X77o_z7-2hlv"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NHkv2J_2hlv"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDoLnSph2hlv"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvAs4oEm2hlv"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkz6UfyD2hlv"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_5010 = temp_sl3_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_5010 = apply_pca_and_reshape(original_fc1_weights_sl3_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_5010 = temp_sl3_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_5010 = apply_pca_and_reshape(original_fc2_weights_sl3_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_5010).float().view(50, 58)\n",
        "PCA_sparse3_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_5010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_5010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP5R1RuC2hlw"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtNzm-WU2hlw"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9Hzk_c2hlw"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVZnQx_Y2hlw"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quDtTGRo2hlw"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fOmNQAc2hlw"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_505 = temp_sl3_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_505 = apply_pca_and_reshape(original_fc1_weights_sl3_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_505 = temp_sl3_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_505 = apply_pca_and_reshape(original_fc2_weights_sl3_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_505).float().view(50, 58)\n",
        "PCA_sparse3_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_505.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_505.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P6P8lkf2hlx"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT2wLg8A2hlx"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liNTggOL2hlx"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGdLhujS2hlx"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7w9EA8E2hlx"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqsWIXHr2hlx"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_3010 = temp_sl3_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_3010 = apply_pca_and_reshape(original_fc1_weights_sl3_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_3010 = temp_sl3_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_3010 = apply_pca_and_reshape(original_fc2_weights_sl3_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_3010).float().view(30, 58)\n",
        "PCA_sparse3_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_3010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_3010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDxHCHNc2hlx"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I214PPHu2hlx"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEqxO5sJ2hly"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUJNjhoF2hly"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SNSv4vD2hly"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA0yX-0F2hly"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_305 = temp_sl3_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_305 = apply_pca_and_reshape(original_fc1_weights_sl3_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_305 = temp_sl3_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_305 = apply_pca_and_reshape(original_fc2_weights_sl3_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_305).float().view(30, 58)\n",
        "PCA_sparse3_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_305.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_305.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tik1jUZ2hly"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QxJEMF42hly"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNavhv9-2hly"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZjQsfVH2hly"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl3_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, temp_sl3_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQWiE0sL2hly"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl3_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl3_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl3_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlADB0tg2hlz"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl3_105 = temp_sl3_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl3_105 = apply_pca_and_reshape(original_fc1_weights_sl3_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl3_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl3_105 = temp_sl3_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl3_105 = apply_pca_and_reshape(original_fc2_weights_sl3_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse3_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse3_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl3_105).float().view(10, 58)\n",
        "PCA_sparse3_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl3_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse3_105.fc1.bias.data.fill_(0)\n",
        "PCA_sparse3_105.fc2.bias.data.fill_(0)\n",
        "PCA_sparse3_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse3_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse3_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse3_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse3_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8KBRzKG2hlz"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse3_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq_WuRAf2hlz"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse3_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1pD8xFMKx9Q"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_sparse4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2t0hQk82ky4"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPLsx12h2ky4"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "042l-8L72ky5"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK475k5w2ky5"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_5030 = temp_sl4_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_5030 = apply_pca_and_reshape(original_fc1_weights_sl4_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_5030 = temp_sl4_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_5030 = apply_pca_and_reshape(original_fc2_weights_sl4_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_5030).float().view(50, 58)\n",
        "PCA_sparse4_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_5030.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_5030.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAt5qgRw2ky5"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHsyRd8t2ky5"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Oldhaj2ky6"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug5VTtkK2ky6"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khbAE72o2ky6"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twpgHxSr2ky6"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_5010 = temp_sl4_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_5010 = apply_pca_and_reshape(original_fc1_weights_sl4_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_5010 = temp_sl4_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_5010 = apply_pca_and_reshape(original_fc2_weights_sl4_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_5010).float().view(50, 58)\n",
        "PCA_sparse4_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_5010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_5010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j4UrOIm2ky6"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg5_no4W2ky7"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pMLLxZS2ky7"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_OoTUkK2ky7"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ8RrBg42ky7"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgh49c7a2ky7"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_505 = temp_sl4_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_505 = apply_pca_and_reshape(original_fc1_weights_sl4_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_505 = temp_sl4_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_505 = apply_pca_and_reshape(original_fc2_weights_sl4_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_505).float().view(50, 58)\n",
        "PCA_sparse4_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_505.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_505.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWT-5efP2ky7"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPw-IZ472ky8"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJ4jMBZ2ky8"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggrF7rgQ2ky8"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQLDxgwM2ky8"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWHoWEo82ky9"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_3010 = temp_sl4_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_3010 = apply_pca_and_reshape(original_fc1_weights_sl4_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_3010 = temp_sl4_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_3010 = apply_pca_and_reshape(original_fc2_weights_sl4_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_3010).float().view(30, 58)\n",
        "PCA_sparse4_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_3010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_3010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngw34Vtc2ky9"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPa3za_32ky-"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSe1OGoH2ky_"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWZJRPhc2ky_"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBhOjge72ky_"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgOrn7RC2kzA"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_305 = temp_sl4_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_305 = apply_pca_and_reshape(original_fc1_weights_sl4_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_305 = temp_sl4_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_305 = apply_pca_and_reshape(original_fc2_weights_sl4_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_305).float().view(30, 58)\n",
        "PCA_sparse4_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_305.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_305.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHWFPZPl2kzA"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llggerbF2kzA"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZt-iOC2kzA"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSwa74PY2kzA"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl4_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, temp_sl4_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha4c1zR-2kzA"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl4_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl4_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl4_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D0sG7M62kzA"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl4_105 = temp_sl4_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl4_105 = apply_pca_and_reshape(original_fc1_weights_sl4_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl4_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl4_105 = temp_sl4_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl4_105 = apply_pca_and_reshape(original_fc2_weights_sl4_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse4_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse4_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl4_105).float().view(10, 58)\n",
        "PCA_sparse4_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl4_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse4_105.fc1.bias.data.fill_(0)\n",
        "PCA_sparse4_105.fc2.bias.data.fill_(0)\n",
        "PCA_sparse4_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse4_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse4_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse4_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse4_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tRsfVno2kzB"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse4_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxxrimGf2kzB"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse4_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRDJ-34MK0ms"
      },
      "source": [
        "#### PCA on the Pruned Model, `pruned_sparse5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQeCkq0J2jFv"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkZmW1A_2jFw"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_5030 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3XakS-_2jFw"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_5030.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_5030.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_5030.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGcXtKd-2jFw"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_5030 = temp_sl5_5030.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_5030 = apply_pca_and_reshape(original_fc1_weights_sl5_5030,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_5030.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_5030 = temp_sl5_5030.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_5030 = apply_pca_and_reshape(original_fc2_weights_sl5_5030,\n",
        "                                                        30)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_5030 = CompressedRegressionNN(58, 50, 30, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_5030.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_5030).float().view(50, 58)\n",
        "PCA_sparse5_5030.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_5030).float().view(30, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_5030.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_5030.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_5030.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_5030.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_5030.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_5030.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_5030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-qmjh9M2jFw"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRPXCa8F2jFw"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_5030, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3uug-Fs2jFx"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FZsLVpd2jFx"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_5010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdCPCtVn2jFx"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_5010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_5010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_5010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDYLkAry2jFx"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_5010 = temp_sl5_5010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_5010 = apply_pca_and_reshape(original_fc1_weights_sl5_5010,\n",
        "                                                        50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_5010.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_5010 = temp_sl5_5010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_5010 = apply_pca_and_reshape(original_fc2_weights_sl5_5010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_5010 = CompressedRegressionNN(58, 50, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_5010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_5010).float().view(50, 58)\n",
        "PCA_sparse5_5010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_5010).float().view(10, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_5010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_5010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_5010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_5010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_5010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_5010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_5010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHNx7W3U2jFx"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zapOxm9X2jFx"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_5010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwh30Ks2jFx"
      },
      "source": [
        "###### fc1 out_features=50 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfVgW3z12jFy"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_505 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SAJdxJX2jFy"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_505.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_505.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_505.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3khoALLf2jFy"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_505 = temp_sl5_505.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_505 = apply_pca_and_reshape(original_fc1_weights_sl5_505,\n",
        "                                                       50)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_505.fc2 = nn.Linear(50, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_505 = temp_sl5_505.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_505 = apply_pca_and_reshape(original_fc2_weights_sl5_505,\n",
        "                                                       5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_505 = CompressedRegressionNN(58, 50, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_505.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_505).float().view(50, 58)\n",
        "PCA_sparse5_505.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_505).float().view(5, 50)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_505.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_505.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_505.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_505.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_505.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_505.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_505)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWzuJG482jFy"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fO2uyCq2jFy"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_505, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhC6vJJR2jFy"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQomT2I82jFy"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_3010 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7smp0nqb2jFy"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_3010.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_3010.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_3010.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLeynBdy2jFz"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_3010 = temp_sl5_3010.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_3010 = apply_pca_and_reshape(original_fc1_weights_sl5_3010,\n",
        "                                                        30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_3010.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_3010 = temp_sl5_3010.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_3010 = apply_pca_and_reshape(original_fc2_weights_sl5_3010,\n",
        "                                                        10)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_3010 = CompressedRegressionNN(58, 30, 10, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_3010.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_3010).float().view(30, 58)\n",
        "PCA_sparse5_3010.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_3010).float().view(10, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_3010.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_3010.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_3010.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_3010.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_3010.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_3010.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_3010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89x8kZpS2jFz"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs0b76wv2jFz"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_3010, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4JZ8ZtN2jFz"
      },
      "source": [
        "###### fc1 out_features=30 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-oBIFxc2jFz"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_305 = RegressionNN(input_size, hidden_size1,\n",
        "                           hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jtbGlou2jFz"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_305.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_305.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_305.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXfF2jkj2jFz"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_305 = temp_sl5_305.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_305 = apply_pca_and_reshape(original_fc1_weights_sl5_305,\n",
        "                                                       30)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_305.fc2 = nn.Linear(30, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_305 = temp_sl5_305.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_305 = apply_pca_and_reshape(original_fc2_weights_sl5_305,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_305 = CompressedRegressionNN(58, 30, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_305.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_305).float().view(30, 58)\n",
        "PCA_sparse5_305.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_305).float().view(5, 30)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_305.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_305.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_305.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_305.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_305.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_305.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_305)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGyfgKOw2jFz"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgiBifRN2jF0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_305, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJGqnkR72jF0"
      },
      "source": [
        "###### fc1 out_features=10 and fc2 out_features=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duROUbiQ2jF0"
      },
      "outputs": [],
      "source": [
        "# create copy of the locally pruned model\n",
        "temp_sl5_105 = RegressionNN(input_size, hidden_size1,\n",
        "                               hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, temp_sl5_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iguojrEr2jF0"
      },
      "outputs": [],
      "source": [
        "# weight shape of the loaded pruned model\n",
        "print(\"fc1 weight shape:\", temp_sl5_105.fc1.weight.shape)\n",
        "print(\"fc2 weight shape:\", temp_sl5_105.fc2.weight.shape)\n",
        "print(\"fc3 weight shape:\", temp_sl5_105.fc3.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OW1OhXh2jF0"
      },
      "outputs": [],
      "source": [
        "# Extract weights from the pruned model\n",
        "original_fc1_weights_sl5_105 = temp_sl5_105.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc1 weights\n",
        "transformed_fc1_weights_sl5_105 = apply_pca_and_reshape(original_fc1_weights_sl5_105,\n",
        "                                                       10)\n",
        "\n",
        "# Update fc2 in the pruned model to match new fc1 out_features\n",
        "temp_sl5_105.fc2 = nn.Linear(10, hidden_size2)\n",
        "\n",
        "# Extract fc2 weights after updating fc2 in the pruned model\n",
        "original_fc2_weights_sl5_105 = temp_sl5_105.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Apply PCA to fc2 weights\n",
        "transformed_fc2_weights_sl5_105 = apply_pca_and_reshape(original_fc2_weights_sl5_105,\n",
        "                                                        5)\n",
        "\n",
        "# Instantiate the compressed model\n",
        "PCA_sparse5_105 = CompressedRegressionNN(58, 10, 5, 1)\n",
        "\n",
        "# Assign the PCA-transformed weights\n",
        "PCA_sparse5_105.fc1.weight.data = torch.from_numpy(transformed_fc1_weights_sl5_105).float().view(10, 58)\n",
        "PCA_sparse5_105.fc2.weight.data = torch.from_numpy(transformed_fc2_weights_sl5_105).float().view(5, 10)\n",
        "\n",
        "# Reset biases\n",
        "PCA_sparse5_105.fc1.bias.data.fill_(0)\n",
        "PCA_sparse5_105.fc2.bias.data.fill_(0)\n",
        "PCA_sparse5_105.fc3.bias.data.fill_(0)\n",
        "\n",
        "# Validate the weight shapes\n",
        "print(\"Transformed fc1 weight shape:\", PCA_sparse5_105.fc1.weight.shape)\n",
        "print(\"Transformed fc2 weight shape:\", PCA_sparse5_105.fc2.weight.shape)\n",
        "print(\"Transformed fc3 weight shape:\", PCA_sparse5_105.fc3.weight.shape)\n",
        "print()\n",
        "print(PCA_sparse5_105)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLub29TD2jF0"
      },
      "outputs": [],
      "source": [
        "average_loss, average_mae = evaluate_model(PCA_sparse5_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDg0iIt2jF0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=PCA_sparse5_105, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYm1tL-KBSZ"
      },
      "source": [
        "## Sparse Principal Components Analysis (SparsePCA) on a Pruned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bcfUaQr6dq7"
      },
      "outputs": [],
      "source": [
        "# function to apply SparsePCA on the input features, in_features\n",
        "# alpha (float) - high value to ensure more sparsity\n",
        "def apply_sparse_pca(layer, n_components, alpha=1.0):\n",
        "\n",
        "    # Extract the layer's weights and transposing them\n",
        "    weights = layer.weight.data.cpu().numpy().T\n",
        "\n",
        "    # Initialize the SparsePCA\n",
        "    # reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA\n",
        "    sparse_pca = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n",
        "\n",
        "    # Fit the SparsePCA model on the transposed weights and transforming them\n",
        "    transformed_weights = sparse_pca.fit_transform(weights)\n",
        "\n",
        "    # Transpose the transformed weights back to their original orientation\n",
        "    return transformed_weights.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp3OQEy_5PA5"
      },
      "source": [
        "### SparsePCA on Locally Pruned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tflz9-RtO-qp"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_local1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBrsDFhOO-q-"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbgwyfApO-q9"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparsePCA_l1_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, sparsePCA_l1_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p95iJWRXVha"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l1_50_transformed_fc1_weights = apply_sparse_pca(sparsePCA_l1_temp.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l1_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l1_50.fc1.weight.data = torch.FloatTensor(l1_50_transformed_fc1_weights).to(sparsePCA_l1_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l1_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l1_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l1_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l1_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxctz5NCXVhn"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l1_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slTq3NbSO-q-"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxPz8VG0YYdv"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparsePCA_l13_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, sparsePCA_l13_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l1_30_transformed_fc1_weights = apply_sparse_pca(sparsePCA_l13_temp.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l1_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA l1_30_transformed weights\n",
        "SparsePCA_l1_30.fc1.weight.data = torch.FloatTensor(l1_30_transformed_fc1_weights).to(sparsePCA_l13_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l1_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l1_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l1_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l1_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DfNvHWsYYd6"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l1_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuB_L-u9O-q-"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgsqGMt_YgKl"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparsePCA_l110_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, sparsePCA_l110_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l1_10_transformed_fc1_weights = apply_sparse_pca(sparsePCA_l110_temp.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l1_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA l1_10_transformed weights\n",
        "SparsePCA_l1_10.fc1.weight.data = torch.FloatTensor(l1_10_transformed_fc1_weights).to(sparsePCA_l110_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l1_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l1_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l1_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l1_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsQHFrb4YgKw"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l1_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1VAe7lYO-q-"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_local2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APJIpEWZO-q-"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBttMsXxXBQC"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l2_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, sparse_l2_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdg_yPZ7YmHS"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l2_50_transformed_fc1_weights = apply_sparse_pca(sparse_l2_temp.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l2_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA l2_50_transformed weights\n",
        "SparsePCA_l2_50.fc1.weight.data = torch.FloatTensor(l2_50_transformed_fc1_weights).to(sparse_l2_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l2_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l2_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l2_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l2_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8qbSy5wYmHb"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l2_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHGedYkfO-q_"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4frj8Md7YxXI"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l230_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, sparse_l230_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l2_30_transformed_fc1_weights = apply_sparse_pca(sparse_l230_temp.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l2_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA l2_30_transformed weights\n",
        "SparsePCA_l2_30.fc1.weight.data = torch.FloatTensor(l2_30_transformed_fc1_weights).to(sparse_l230_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l2_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l2_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l2_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l2_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5rUljPYYxXS"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l2_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o3kxsooO-q_"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM0rggyqY2zF"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l210_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, sparse_l210_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l2_10_transformed_fc1_weights = apply_sparse_pca(sparse_l210_temp.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l2_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA l2_10_transformed weights\n",
        "SparsePCA_l2_10.fc1.weight.data = torch.FloatTensor(l2_10_transformed_fc1_weights).to(sparse_l210_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l2_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l2_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l2_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l2_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOwTQeW_Y2zG"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l2_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noMqmdrLO-q_"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_local3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIq96y7OO-q_"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCyuqacHXFvP"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l3_temp = RegressionNN(input_size, hidden_size1, hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, sparse_l3_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_aw-pJMZWDW"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l3_50_transformed_fc1_weights = apply_sparse_pca(sparse_l3_temp.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l3_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l3_50.fc1.weight.data = torch.FloatTensor(l3_50_transformed_fc1_weights).to(sparse_l3_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l3_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l3_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l3_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l3_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pcj1zVGZWDk"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l3_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRrKpX-SO-rA"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdTmY_pFZOTT"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l330_temp = RegressionNN(input_size, hidden_size1, hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, sparse_l330_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l3_30_transformed_fc1_weights = apply_sparse_pca(sparse_l330_temp.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l3_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l3_30.fc1.weight.data = torch.FloatTensor(l3_30_transformed_fc1_weights).to(sparse_l330_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l3_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l3_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l3_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l3_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue4YWlj8ZOTf"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l3_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4hYcdriO-rA"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SmiT6VNZEVG"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l310_temp = RegressionNN(input_size, hidden_size1, hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, sparse_l310_temp)\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l3_10_transformed_fc1_weights = apply_sparse_pca(sparse_l310_temp.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l3_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l3_10.fc1.weight.data = torch.FloatTensor(l3_10_transformed_fc1_weights).to(sparse_l310_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l3_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l3_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l3_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l3_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKLHMXDPZEVR"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l3_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JGhX_AO-rA"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_local4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqbUYkgWO-rA"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzS6ogf9XJOr"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l4_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, sparse_l4_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxsi3DtDZ5_8"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l4_50_transformed_fc1_weights = apply_sparse_pca(sparse_l4_temp.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l4_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l4_50.fc1.weight.data = torch.FloatTensor(l4_50_transformed_fc1_weights).to(sparse_l4_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l4_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l4_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l4_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l4_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxuB4aCYZ6AG"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l4_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEiegWqLO-rA"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2CIDOtMZsPU"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l430_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, sparse_l430_temp)\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l4_30_transformed_fc1_weights = apply_sparse_pca(sparse_l430_temp.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l4_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                       fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l4_30.fc1.weight.data = torch.FloatTensor(l4_30_transformed_fc1_weights).to(sparse_l430_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l4_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l4_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l4_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l4_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqT3b3nqZsPf"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l4_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUjm1d0mO-rA"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIlTY5sMZkLH"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l410_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, sparse_l410_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l4_10_transformed_fc1_weights = apply_sparse_pca(sparse_l410_temp.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l4_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l4_10.fc1.weight.data = torch.FloatTensor(l4_10_transformed_fc1_weights).to(sparse_l410_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l4_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l4_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l4_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l4_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ8M8n21ZkLH"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l4_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PimdwWXCO-rB"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_local5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4HqZvZLO-rB"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja0NM7mmXNc4"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l5_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, sparse_l5_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWAD6rCzaBgO"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l5_50_transformed_fc1_weights = apply_sparse_pca(sparse_l5_temp.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l5_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l5_50.fc1.weight.data = torch.FloatTensor(l5_50_transformed_fc1_weights).to(sparse_l5_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l5_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l5_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l5_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l5_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NJA3FNDaBgX"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l5_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSq1UFIpO-rB"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glKZ07r5aHx3"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l530_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, sparse_l530_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l5_30_transform_fc1_weights = apply_sparse_pca(sparse_l530_temp.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l5_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l5_30.fc1.weight.data = torch.FloatTensor(l5_30_transform_fc1_weights).to(sparse_l530_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l5_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l5_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l5_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l5_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0hzq3SbaHyE"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l5_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIKVll3dO-rB"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn5DmiVFaP02"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "sparse_l510_temp = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, sparse_l510_temp)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "l5_10_transformed_fc1_weights = apply_sparse_pca(sparse_l510_temp.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_l5_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_l5_10.fc1.weight.data = torch.FloatTensor(l5_10_transformed_fc1_weights).to(sparse_l510_temp.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_l5_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_l5_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_l5_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_l5_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYaMueX6aP1A"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_l5_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGXCgbSfM2IP"
      },
      "source": [
        "### SparsePCA on Layer-Wise Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGDi1-kBM2IP"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_sparse1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pf0BsUzM2IQ"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjc1iNj9abYn"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse1 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, SparsePCA_sparse1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZL7-Z-HaixA"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse1.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse1_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse1_50.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse1.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse1_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse1_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St5eRNCIaixL"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse1_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSn1O0aVM2IQ"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gInidUsNa0vZ"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse130 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, SparsePCA_sparse130)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse130.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse1_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse1_30.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse130.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse1_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse1_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxPByMmDa0vi"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse1_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE6kUf-SM2IR"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLnnyOzGbDNR"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse110 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, SparsePCA_sparse110)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse110.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse1_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse1_10.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse110.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse1_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse1_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse1_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wehl9SKbbDNa"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse1_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUWAp59M2IR"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_sparse2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyGOZ2rDbgMY"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6yZBRDGbgMO"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse2 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, SparsePCA_sparse2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSY19Eg_bgMY"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse2.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse2_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse2_50.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse2.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse2_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse2_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eMvuzTybgMY"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse2_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LRHpLr2bgMY"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbMJk9jYbgMZ"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse230 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, SparsePCA_sparse230)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse230.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse2_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse2_30.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse230.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse2_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse2_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDXmtM3AbgMZ"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse2_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PRvEAwbgMZ"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa6gouLebgMZ"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse210 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, SparsePCA_sparse210)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse210.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse2_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse2_10.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse210.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse2_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse2_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse2_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGhr5ZjrbgMZ"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse2_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hjv4KAlM2IS"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_sparse3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxG0F7-9biSN"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjKScqpcbiSM"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse3 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, SparsePCA_sparse3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEWvTRbTbiSN"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse3.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse3_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse3_50.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse3.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse3_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse3_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbFLdpuJbiSN"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse3_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yPgkKGDbiSN"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py9VuUGObiSN"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse330 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, SparsePCA_sparse330)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse330.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse3_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse3_30.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse330.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse3_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse3_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx9wP-Z5biSO"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse3_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VmDnyD4biSO"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnp4DaKEbiSO"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse310 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, SparsePCA_sparse310)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse310.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse3_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse3_10.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse310.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse3_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse3_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse3_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZrmsn_GbiSO"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse3_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNrarkXvM2IT"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_sparse4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dNyAVvpbnrl"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncH9lSumbnrb"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse4 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, SparsePCA_sparse4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_temniZcbnrl"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse4.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse4_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse4_50.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse4.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse4_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse4_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODFfyL6ibnrl"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse4_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUnAI4Lzbnrl"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKZ-PBfJbnrl"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse430 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, SparsePCA_sparse430)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse430.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse4_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse4_30.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse430.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse4_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse4_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wqbSxWbbnrl"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse4_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOIDxAg_bnrl"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmMyDJgbbnrm"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse410 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, SparsePCA_sparse410)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse410.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse4_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse4_10.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse410.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse4_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse4_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse4_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SdtGfMRbnrm"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse4_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWQzUSx2M2IU"
      },
      "source": [
        "#### SparsePCA on the Pruned Model, `pruned_sparse5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqXxt8PhbmZo"
      },
      "source": [
        "##### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOBk4mOwbmZe"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse5 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, SparsePCA_sparse5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1iq_0BWbmZz"
      },
      "outputs": [],
      "source": [
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse5.fc1,\n",
        "                                           n_components=50, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 50\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse5_50 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse5_50.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse5.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse5_50.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_50.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_50.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse5_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGypPDoZbmZz"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse5_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfq7c6KbmZz"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GorxxHv0bmZ0"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse530 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, SparsePCA_sparse530)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse530.fc1,\n",
        "                                           n_components=30, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 30\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse5_30 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse5_30.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse530.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse5_30.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_30.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_30.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse5_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hgzt3UtbmZ0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse5_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRaZimDtbmZ0"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxIlK76EbmZ0"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "SparsePCA_sparse510 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, SparsePCA_sparse510)\n",
        "\n",
        "# Apply SparsePCA to fc1 and fc2 of the pruned model\n",
        "transformed_fc1_weights = apply_sparse_pca(SparsePCA_sparse510.fc1,\n",
        "                                           n_components=10, alpha=0.5)\n",
        "\n",
        "# the adjusted model\n",
        "fc1_out_features = 10\n",
        "fc2_out_features = 32\n",
        "SparsePCA_sparse5_10 = CompressedRegressionNN(input_size, fc1_out_features,\n",
        "                                          fc2_out_features, output_size)\n",
        "\n",
        "# Update the adjusted model with SparsePCA transformed weights\n",
        "SparsePCA_sparse5_10.fc1.weight.data = torch.FloatTensor(transformed_fc1_weights).to(SparsePCA_sparse510.fc1.weight.device)\n",
        "\n",
        "# Reset biases to zero because they are not specifically adjusted post-PCA\n",
        "SparsePCA_sparse5_10.fc1.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_10.fc2.bias.data.fill_(0)\n",
        "SparsePCA_sparse5_10.fc3.bias.data.fill_(0)\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(SparsePCA_sparse5_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUAUfauHbmZ0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=SparsePCA_sparse5_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_CLaIudc0oT"
      },
      "source": [
        "## Independent Component Analysis (ICA) for a Pruned Model\n",
        "\n",
        "ICA can be applied to pruned model weights in a similar manner to PCA. The flattened weights from each linear layer are stacked into a matrix and ICA is applied on it to obtain the transformed weights in ica_result.\n",
        "\n",
        "Reference:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SntG5VcO74Xp"
      },
      "outputs": [],
      "source": [
        "# defining a function to apply ICA on the pruned model\n",
        "def apply_ica_to_layer(layer, n_components):\n",
        "    # interested in reducing the dimensionality of in_features\n",
        "    # Transpose to focus on in_features\n",
        "    weights = layer.weight.data.cpu().numpy().T\n",
        "    # reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA\n",
        "    ica = FastICA(n_components=n_components, random_state=0)\n",
        "    transformed_weights = ica.fit_transform(weights)\n",
        "    # Transpose back to match original shape orientation\n",
        "    return transformed_weights.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZ3u9oczzlc"
      },
      "outputs": [],
      "source": [
        "# Define a new adjusted model with dimensions matching the ICA output\n",
        "class AdjustedModel(RegressionNN):\n",
        "    def __init__(self):\n",
        "        super().__init__(input_size=58,\n",
        "                         hidden_size1=n_components_fc1,\n",
        "                         hidden_size2=32,\n",
        "                         output_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XflsFQqqYn0i"
      },
      "source": [
        "### ICA on Locally Pruned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hm6f0PvTpb"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_local1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlKkWUfLPLio"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U31R2kQOwAn5"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local1= RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, ICA_pruned_local1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpgYMKZXet-7"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "ical150_transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local1.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l1_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l1_50.fc1.weight.data = torch.from_numpy(ical150_transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l1_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a6L4T7-et-7"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l1_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch4qTIo9PLip"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWCHEpYBezSy"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local130= RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, ICA_pruned_local130)\n",
        "\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "ical130_transofrmed_fc1_weights = apply_ica_to_layer(ICA_pruned_local130.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l1_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l1_30.fc1.weight.data = torch.from_numpy(ical130_transofrmed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l1_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56hjdVayezSz"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l1_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vky7QuhHPLip"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIiks7tNz5ry"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local110= RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local1, ICA_pruned_local110)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "ical110_transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local110.fc1,\n",
        "                                                     n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l1_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l1_10.fc1.weight.data = torch.from_numpy(ical110_transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l1_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scRWMtL0ejK2"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l1_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nXyCjDBPEz0"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_local2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otB-BcL8fE1X"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7wYsg8afE1M"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local2 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, ICA_pruned_local2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRyGlXUrfE1X"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "ical250_transofrmed_fc1_weights = apply_ica_to_layer(ICA_pruned_local2.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l2_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l2_50.fc1.weight.data = torch.from_numpy(ical250_transofrmed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l2_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvNJxnhQfE1Y"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l2_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erElya6CfE1Y"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQq43zeZfE1Y"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local230 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, ICA_pruned_local230)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "ical230_transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local230.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l2_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l2_30.fc1.weight.data = torch.from_numpy(ical230_transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l2_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3G7iGvsfE1Z"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l2_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wwQffeHfE1Z"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1M8YRxnfE1Z"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local_210 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local2, ICA_local_210)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "ical210_transformed_fc1_weights = apply_ica_to_layer(ICA_local_210.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l2_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l2_10.fc1.weight.data = torch.from_numpy(ical210_transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l2_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGwk8nMUfE1Z"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l2_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqOn4gWiPE0L"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_local3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MsafvHKfGaz"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIkHU2pcfGay"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local_08 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, ICA_pruned_local_08)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXu7aExnfGaz"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local_08.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l3_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l3_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l3_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Aov0sLfGa0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l3_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDOz8qghfGa0"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK2-siIxfGa0"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local330 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, ICA_local330)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local330.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l3_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l3_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l3_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl4SVagMfGa0"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l3_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-96YfwmfGa1"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEvGPPJufGa1"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local310 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local3, ICA_local310)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local310.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l3_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l3_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l3_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csYUgh61fGa1"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l3_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yc6IgOEPE0L"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_local4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r10CXRrfHfl"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvoX4RQOfHfk"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local_09 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, ICA_pruned_local_09)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sdngcncfHfl"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local_09.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l4_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l4_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l4_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q44hTiGfHfm"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l4_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTorXr6AfHfm"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk8y25U6fHfm"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local430 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, ICA_local430)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local430.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l4_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l4_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l4_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_4zQg9bfHfm"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l4_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2D-KLTvfHfn"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWbTPQ7zfHfn"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local410 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local4, ICA_local410)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local410.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l4_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l4_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l4_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4WZ3EeYfHfn"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l4_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-2k3KifPE0M"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_local5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D60WhUXifIT1"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOq_P2lKfIT0"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_pruned_local_09 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, ICA_pruned_local_09)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kkugkivfIT2"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_pruned_local_09.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l5_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l5_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l5_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYnbo34lfIT2"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l5_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFV-tNm9fIT2"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF-mVlZ6fIT3"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local530 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, ICA_local530)\n",
        "\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local530.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l5_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l5_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l5_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnF9rao3fIT3"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l5_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHoYYVE6fIT3"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjHKiDBJfIT3"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_local530 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_local5, ICA_local530)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_local530.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_l5_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_l5_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_l5_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEdaeBJxfIT3"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_l5_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyt6bm7hNNyF"
      },
      "source": [
        "### ICA on Layer-Wise Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqS4W_y1NNyQ"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_sparse1`, with Pruning Amount=0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxpjtHRfMY4"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8QToV_sfMYs"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse1 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, ICA_sparse1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSxTVXVXfMY4"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse1.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse1_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse1_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse1_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOCF3tgcfMY4"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse1_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-zKIV7lfMY4"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP7m6HB9fMY5"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse130 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, ICA_sparse130)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse130.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse1_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse1_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse1_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHvTIDNnfMY5"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse1_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHuxaD0BfMY5"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj1mewAAfMY5"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse110 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse1, ICA_sparse110)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse110.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse1_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse1_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse1_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLW07W9JfMY5"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse1_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU4OlVSMNNyS"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_sparse2`, with Pruning Amount=0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz3-E4b5hqaS"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNWinXo8hqaR"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse2 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, ICA_sparse2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rslpmbrjhqaT"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse2.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse2_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse2_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse2_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zVhdgVphqaU"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse2_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5wPQgRyhqaV"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcu2LWIbhqaW"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse230 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, ICA_sparse230)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse230.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse2_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse2_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse2_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHd5B6y0hqaW"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse2_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEOR-eQhqaW"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjx6J1SnhqaX"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse210 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse2, ICA_sparse210)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse210.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse2_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse2_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse2_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNIheG7YhqaX"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse2_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iWp7tK5NNyT"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_sparse3`, with Pruning Amount=0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOfwQiGUhrbm"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbW47DjThrbl"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse3 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, ICA_sparse3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQy4A4vphrbn"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse3.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse3_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse3_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse3_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOvO2pObhrbn"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse3_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzE3x3Ozhrbn"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz00cAgvhrbn"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse330 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, ICA_sparse330)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse330.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse3_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse3_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse3_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxPBeyBShrbn"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse3_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHJ5kJVWhrbo"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ-5qinqhrbo"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse310 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse3, ICA_sparse310)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse310.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse3_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse3_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse3_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAbSf6Xghrbo"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse3_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCEWRmm4NNyU"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_sparse4`, with Pruning Amount=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eme_kV46hsQY"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzsQR8IxhsQX"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse4 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, ICA_sparse4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkVFV3OehsQZ"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse4.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse4_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse4_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse4_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJt2va6ehsQZ"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse4_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaL62-zhhsQZ"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbRWnO5khsQa"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse430 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, ICA_sparse430)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse430.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse4_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse4_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse4_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cInxAbhhsQa"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse4_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qxM5MwIhsQb"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYOlhCj7hsQb"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse410 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse4, ICA_sparse410)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse410.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse4_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse4_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse4_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVJlfh7hsQb"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse4_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "molc3QHMNNyW"
      },
      "source": [
        "#### ICA on the Pruned Model, `pruned_sparse5`, with Pruning Amount=0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYCzEJG8htc9"
      },
      "source": [
        "###### no_components = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIawsposhtcw"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse5 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, ICA_sparse5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPjXoJ5nhtc9"
      },
      "outputs": [],
      "source": [
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 50  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse5.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse5_50 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse5_50.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse5_50, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSF4Ti-Ghtc9"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse5_50, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfTzeelhtc-"
      },
      "source": [
        "##### no_components = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lvLMhUthtc-"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse530 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, ICA_sparse530)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 30  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse530.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse5_30 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse5_30.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse5_30, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93kWTzZkhtc-"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse5_30, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv9qclT0htc-"
      },
      "source": [
        "##### no_components = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJX6EEmlhtc_"
      },
      "outputs": [],
      "source": [
        "# create copy of the model\n",
        "ICA_sparse510 = RegressionNN(input_size, hidden_size1,\n",
        "                                         hidden_size2, output_size)\n",
        "transfer_pruned_weights(pruned_sparse5, ICA_sparse510)\n",
        "\n",
        "# Apply ICA to the first layer as an example\n",
        "n_components_fc1 = 10  # New dimension for fc1\n",
        "transformed_fc1_weights = apply_ica_to_layer(ICA_sparse510.fc1,\n",
        "                                             n_components=n_components_fc1)\n",
        "\n",
        "# Instantiate the adjusted model\n",
        "ICA_sparse5_10 = AdjustedModel()\n",
        "\n",
        "# Update the adjusted model with ICA transformed weights for fc1\n",
        "ICA_sparse5_10.fc1.weight.data = torch.from_numpy(transformed_fc1_weights).float()\n",
        "\n",
        "# testing without training\n",
        "average_loss, average_mae = evaluate_model(ICA_sparse5_10, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP1AVEclhtdA"
      },
      "outputs": [],
      "source": [
        "# train and validate the model with PCA = 30\n",
        "trained_model, training_losses, validation_losses = train_and_validate(\n",
        "   model=ICA_sparse5_10, train_loader=train_loader,\n",
        "   test_loader=test_loader, learning_rate=0.001, epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAceBjlInMlj"
      },
      "source": [
        "## Comparison of All Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTmoQEIMpAv"
      },
      "source": [
        "### Creating and Training Smaller Networks\n",
        "\n",
        "In this section, we will create smaller networks to match the dimensions of our compressed pruned models (PCA, SparsePCA, and ICA).  We are doing this to compare the performance of the original, smaller, and compressed network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3pXf4qJOO4E"
      },
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "input_size = 58  # Number of input features\n",
        "output_size = 1\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gu7zc_dQRsJ"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=50 and fc2=32\n",
        "small_5032 = RegressionNN(input_size, 50, 32, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_5032, training_losses_5032, validation_losses_5032 = train_and_validate(\n",
        "   model=small_5032, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_5032, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CflfHL1HNoO9"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=50 and fc2=30\n",
        "small_5030 = RegressionNN(input_size, 50, 30, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_5030, training_losses_5030, validation_losses_5030 = train_and_validate(\n",
        "   model=small_5030, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_5030, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8rthp2OOHd5"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=50 and fc2=10\n",
        "small_5010 = RegressionNN(input_size, 50, 10, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_5010, training_losses_5010, validation_losses_5010 = train_and_validate(\n",
        "   model=small_5010, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_5010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxU4HOPIOHeG"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=50 and fc2=5\n",
        "small_505 = RegressionNN(input_size, 50, 5, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_505, training_losses_505, validation_losses_505 = train_and_validate(\n",
        "   model=small_505, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_505, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uk9NbBDRjSd"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=30 and fc2=32\n",
        "small_3032 = RegressionNN(input_size, 30, 32, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_3032, training_losses_3032, validation_losses_3032 = train_and_validate(\n",
        "   model=small_3032, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_3032, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnFDR2FlRtiu"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=30 and fc2=10\n",
        "small_3010 = RegressionNN(input_size, 30, 10, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_3010, training_losses_3010, validation_losses_3010 = train_and_validate(\n",
        "   model=small_3010, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_3010, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEG8aPSbRyhb"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=30 and fc2=5\n",
        "small_305 = RegressionNN(input_size, 30, 5, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_305, training_losses_305, validation_losses_305 = train_and_validate(\n",
        "   model=small_305, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_305, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQuL6AAaSRKd"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=10 and fc2=32\n",
        "small_1032 = RegressionNN(input_size, 10, 32, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_1032, training_losses_1032, validation_losses_1032 = train_and_validate(\n",
        "   model=small_1032, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_1032, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQsaWjkTR7Ki"
      },
      "outputs": [],
      "source": [
        "# smaller network with fc1=10 and fc2=5\n",
        "small_105 = RegressionNN(input_size, 10, 5, output_size)\n",
        "# Call train_and_validate function\n",
        "trained_model_105, training_losses_105, validation_losses_105 = train_and_validate(\n",
        "   model=small_105, train_loader=train_loader,test_loader=test_loader,\n",
        "   learning_rate=learning_rate, epochs=epochs\n",
        ")\n",
        "# Average Loss and MAE\n",
        "average_loss, average_mae = evaluate_model(small_105, test_loader, device)\n",
        "print(f'Average Loss: {average_loss:.4f}, Average MAE: {average_mae:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBonzPWkP-tY"
      },
      "source": [
        "### Calculating the MSE and MAE losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2-JaN__a3sp"
      },
      "source": [
        "#### Original, Local, and Locally Compressed (PCA, SparsePCA, ICA) Pruned Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHoEcXY8nPiy"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.6 pruning amount\n",
        "local1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_local1,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local1_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local1_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local1_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local1_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local1_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l1_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l1_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l1_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l1_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_local1 = {}\n",
        "\n",
        "for name, model in local1_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_local1[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs82a03LYUL3"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.7 pruning amount\n",
        "local2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_local2,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local2_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l2_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l2_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l2_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l2_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_local2 = {}\n",
        "\n",
        "for name, model in local2_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_local2[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxngYng0Yh_A"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.8 pruning amount\n",
        "local3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_local3,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local3_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local3_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local3_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local3_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local3_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l3_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l3_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l3_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l3_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_local3 = {}\n",
        "\n",
        "for name, model in local3_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_local3[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcCRNpSQYhmv"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.9 pruning amount\n",
        "local4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_local4,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local4_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local4_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local4_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local4_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local4_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l4_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l4_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l4_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l4_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_local4 = {}\n",
        "\n",
        "for name, model in local4_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_local4[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C4ozDy5YhSF"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.95 pruning amount\n",
        "local5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_local5,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local5_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local5_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local5_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local5_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local5_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l5_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l5_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l5_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l5_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_local5 = {}\n",
        "\n",
        "for name, model in local5_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_local5[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ7U0t4bbI8-"
      },
      "source": [
        "#### Original, Local, and Layer-wise Locally Compressed (PCA, SparsePCA, ICA) Pruned Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhYeY0zQZ14c"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.6 pruning amount\n",
        "sparse1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse1,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse1_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse1_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse1_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse1_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse1_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse1_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse1_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse1_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse1_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse1 = {}\n",
        "\n",
        "for name, model in sparse1_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse1[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDK9A4eaaHmZ"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.6 pruning amount\n",
        "sparse2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse2,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse2_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse2_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse2_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse2_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse2_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse2 = {}\n",
        "\n",
        "for name, model in sparse2_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse2[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHTpzSlSaIlG"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.7 pruning amount\n",
        "sparse2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse2,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse2_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse2_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse2_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse2_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse2_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse2 = {}\n",
        "\n",
        "for name, model in sparse2_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse2[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woScwgrfaSmM"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.8 pruning amount\n",
        "sparse3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse3,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse3_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse3_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse3_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse3_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse3_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse3_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse3_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse3_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse3_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse3 = {}\n",
        "\n",
        "for name, model in sparse3_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse3[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE-rDScvaSL-"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.9 pruning amount\n",
        "sparse4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse4,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse4_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse4_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse4_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse4_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse4_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse4_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse4_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse4_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse4_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse4 = {}\n",
        "\n",
        "for name, model in sparse4_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse4[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-XagbMuaR1x"
      },
      "outputs": [],
      "source": [
        "# checking layer-wise pruned models that have 0.95 pruning amount\n",
        "sparse5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"Local Pruned\": pruned_sparse5,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse5_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse5_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse5_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse5_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse5_105,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse5_10,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse5_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse5_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse5_10,\n",
        "\n",
        "}\n",
        "\n",
        "results_sparse5 = {}\n",
        "\n",
        "for name, model in sparse5_models.items():\n",
        "    loss, mae = evaluate_model(model, test_loader, device)\n",
        "    results_sparse5[name] = {\"MSE Loss\": loss, \"MAE\": mae}\n",
        "    print(f\"{name} - MSE Loss: {loss:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK03mXOQz79p"
      },
      "source": [
        "### Weight Distribution in each Layer of the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUfo7RLeNqCh"
      },
      "source": [
        "#### Inspecting the Weight Distribution of the Original and Pruned Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Vpr4rF4qpZ"
      },
      "outputs": [],
      "source": [
        "# list of local and pruned models (without PCA, SparsePCA, and ICA)\n",
        "local_pruneList = {\n",
        "    \"Original\": model,\n",
        "    \"Local Pruned - 0.60\": pruned_local1,\n",
        "    \"Local Pruned - 0.70\": pruned_local2,\n",
        "    \"Local Pruned - 0.80\": pruned_local3,\n",
        "    \"Local Pruned - 0.90\": pruned_local4,\n",
        "    \"Local Pruned - 0.95\": pruned_local5,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFdc0nF59l1"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in local_pruneList.items():\n",
        "      # reference: https://stackoverflow.com/questions/44938160/saving-layer-weights-at-each-epoch-during-training-into-a-numpy-type-array-conv\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=125)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(local_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQFsR8yR69mf"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in local_pruneList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=125)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(local_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzX9gHUM7pX5"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in local_pruneList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=125)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(local_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPZFk0DRBFBZ"
      },
      "source": [
        "#### Inspecting the Weight Distribution of the Original and Smaller Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4GBQ2y2BFBi"
      },
      "outputs": [],
      "source": [
        "# list of local and pruned models (without PCA, SparsePCA, and ICA)\n",
        "OG_small_list = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjP8l0xGBFBi"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in OG_small_list.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=125)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(OG_small_list.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPeEPOd-BFBi"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in OG_small_list.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(OG_small_list.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ3dI-PvB8Qr"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in OG_small_list.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(OG_small_list.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-Saa7X8KZa"
      },
      "source": [
        "#### Inspecting the Weight Distribution of the PCA Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgwtNU2KFR1m"
      },
      "source": [
        "##### PCA on Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wBa2iuFaRX"
      },
      "source": [
        "###### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCmMGRR78KZb"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_pruneList = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local1_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local1_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local1_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local1_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local1_105\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUYxpElu_XJs"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gynCEBCJ_W5q"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOyI1zw8KZd"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=35)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIZ8Q2biIZ6y"
      },
      "source": [
        "###### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CjQKpwHIZ7A"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_pruneList2 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local2_105\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsO-tqLKIZ7A"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIosghc4IZ7A"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s86esFCfIZ7B"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=35)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1iXvNphIaxk"
      },
      "source": [
        "###### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtJ1QhvwIaxl"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_pruneList3 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local3_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local3_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local3_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local3_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local3_105\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p58QiqtIaxl"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR1G7elKIaxm"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cifGF_HIaxm"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=35)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf-ltz2PIbae"
      },
      "source": [
        "###### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmlWuWvzIbaf"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_pruneList4 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local4_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local4_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local4_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local4_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local4_105\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWa3B3eLIbaf"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM0uDrDkIbaf"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_FcSRPLIbaf"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=35)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syP9Owb9IcEI"
      },
      "source": [
        "###### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU0RIM3BIcEJ"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_pruneList5 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local5_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local5_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local5_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local5_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local5_105\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HNPmi6IIcEJ"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMzNmAhIcEJ"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIUea4gQIcEJ"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_pruneList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=35)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTqrgzcPFkoR"
      },
      "source": [
        "##### PCA on Layer-wise Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4-MvLCxFnj4"
      },
      "source": [
        "###### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp6a42ZR870Q"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_layerwiseList = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse1_5010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse1_505,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse1_3010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse1_305,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse1_105\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900beq-69V6P"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ro1SryQ_0FM"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idWjYKR39V6Y"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_CNVx3dF2wo"
      },
      "source": [
        "###### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71vbjMYyF2w5"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_layerwiseList2 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse2_5010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse2_505,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse2_3010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse2_305,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse2_105\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvSbd3HVF2w6"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0v3DPppF2w6"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqv7uIfdF2w7"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQtdWoiYF3eU"
      },
      "source": [
        "###### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA0dMpYuF3eU"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_layerwiseList3 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse3_5010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse3_505,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse3_3010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse3_305,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse3_105\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyiZuTw3F3eV"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXOvEU9lF3eV"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYv0VLm0F3eV"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList3.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij7S8FuLF4Q9"
      },
      "source": [
        "###### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u_QuaM0F4Q-"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_layerwiseList4 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse4_5010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse4_505,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse4_3010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse4_305,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse4_105\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekvVYkfiF4Q-"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atw72NJkF4Q-"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2dm9cyBF4Q-"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList4.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4BjMWoDF4-p"
      },
      "source": [
        "###### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBODAa8DF4-q"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "PCA_layerwiseList5 = {\n",
        "    \"Original\": model,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse5_5010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse5_505,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse5_3010,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse5_305,\n",
        "    \"PCA Layer-Wise Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse5_105\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXEPEbnjF4-q"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dThA-fxF4-q"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlhm8yN3F4-q"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in PCA_layerwiseList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=10)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(PCA_layerwiseList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3M7VvhE9t5t"
      },
      "source": [
        "#### Inspecting the Weight Distribution of the SparsePCA Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6_HStI4JkXZ"
      },
      "source": [
        "##### SparsePCA on Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnSZLDRkJmFm"
      },
      "source": [
        "###### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjZFvf5-9t8i"
      },
      "outputs": [],
      "source": [
        "# list of SparsePCA local models\n",
        "SparsePCA_pruneList = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l1_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-as380o9t8j"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0emhahOA9t8j"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0_-4NQIALMK"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=20)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShRIoPHQJ6GQ"
      },
      "source": [
        "###### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrvng74SJ6Gc"
      },
      "outputs": [],
      "source": [
        "# list of SparsePCA local models\n",
        "SparsePCA_pruneList2 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l2_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-VCr4KeJ6Gf"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList2.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqZa3L57J6Gg"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList2.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxKzKu66J6Gh"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList2.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=20)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WSd-HldJ7pz"
      },
      "source": [
        "###### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdPb1UoKJ7qS"
      },
      "outputs": [],
      "source": [
        "# list of SparsePCA local models\n",
        "SparsePCA_pruneList3 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l3_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqfA4-qMJ7qT"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuINEIkPJ7qZ"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njcYY7aBJ7qZ"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=20)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2AvzrIJ8xh"
      },
      "source": [
        "###### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5I4fXKpJ8xi"
      },
      "outputs": [],
      "source": [
        "# list of SparsePCA local models\n",
        "SparsePCA_pruneList4 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l4_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKvAanpXJ8xj"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WpnvQ-sJ8xk"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AktQtQQpJ8xk"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=20)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHxn15yPJ92t"
      },
      "source": [
        "###### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQQhBdAtJ92t"
      },
      "outputs": [],
      "source": [
        "# list of SparsePCA local models\n",
        "SparsePCA_pruneList5 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l5_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcMiF6WpJ92t"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList5.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=200)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw_gyIYrJ92u"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList5.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcVO1kCSJ92u"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_pruneList5.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=20)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_pruneList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYUaDWiPJuVY"
      },
      "source": [
        "##### SparsePCA on Layer-wise Locally Pruned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "316hB1TwJxtQ"
      },
      "source": [
        "###### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1khe71zG9t8n"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "SparsePCA_spasreList = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse1_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07M1ZssC9t8v"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyM47PLT9t9D"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=80)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgA8GyzsAbIl"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=15)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVQg7Z03Kx1g"
      },
      "source": [
        "###### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPnLzoppKx1s"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "SparsePCA_spasreList2 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse2_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVt1Fez2Kx1t"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A29ibGpWKx1t"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=80)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-gd-iP7Kx1t"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList2.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=15)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList2.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vyrw-f9KysF"
      },
      "source": [
        "###### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR3c0nJXKysF"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "SparsePCA_spasreList3 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse3_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzfTHFC_KysG"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwG4qJs0KysG"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=80)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGmn6NXwKysG"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList3.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=15)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList3.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7h5pj6NKzlO"
      },
      "source": [
        "###### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQoiI6deKzli"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "SparsePCA_spasreList4 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse4_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5C5EEBoKzli"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgFU6B-TKzlj"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=80)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhhbZc9vKzlj"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList4.items():\n",
        "        # Flatten the weight data and convert to CPU numpy array\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=15)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList4.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2o5hJ8XK0d1"
      },
      "source": [
        "###### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzX1srsRK0d2"
      },
      "outputs": [],
      "source": [
        "# list of PCA local models\n",
        "SparsePCA_spasreList5 = {\n",
        "    \"Original\": model,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse5_10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJTEod3iK0d2"
      },
      "outputs": [],
      "source": [
        "# Define fc1 layers\n",
        "layers = ['fc1']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc1']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=100)\n",
        "ax.set_title(\"Weight Distribution of fc1\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11xBaF9RK0d3"
      },
      "outputs": [],
      "source": [
        "# Define fc2 layers\n",
        "layers = ['fc2']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc2']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=80)\n",
        "ax.set_title(\"Weight Distribution of fc2\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QH31zdMK0d4"
      },
      "outputs": [],
      "source": [
        "# Define fc3 layers\n",
        "layers = ['fc3']\n",
        "\n",
        "weight_data = {layer: [] for layer in layers}\n",
        "\n",
        "# Collecting weight data\n",
        "for layer in layers:\n",
        "    for model_name, model in SparsePCA_spasreList5.items():\n",
        "        weights = getattr(model, layer).weight.data.reshape(-1).cpu().numpy()\n",
        "        weight_data[layer].append(weights)\n",
        "\n",
        "# Create subplots for each layer\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "for weights in weight_data['fc3']:\n",
        "    ax.hist(weights, bins=100, alpha=0.5)\n",
        "ax.set_ylim(top=15)\n",
        "ax.set_title(\"Weight Distribution of fc3\")\n",
        "ax.set_xlabel(\"Weight Value\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend(SparsePCA_spasreList5.keys())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Bp-hVV09qC"
      },
      "source": [
        "### Predictive Performance Plots across all the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_JEtjsAfF6"
      },
      "source": [
        "#### Original, Local, and Locally Pruned Compressed (PCA, SparsePCA, ICA) Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7MZa_vzAWNQ"
      },
      "source": [
        "##### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClWKs0mo7J4C"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.6 pruning amount\n",
        "PCA_l1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local1_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local1_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local1_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local1_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local1_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_l1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    # reference: https://stackoverflow.com/questions/40776069/python-convert-prediction-result-into-one-hot\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krwm-oYu7xz7"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.6 pruning amount\n",
        "SparsePCA_l1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l1_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdznIDq7xOV"
      },
      "outputs": [],
      "source": [
        "# checking local models that have 0.6 pruning amount\n",
        "ICA_l1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l1_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l1_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l1_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_l1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6iXwKmCAylP"
      },
      "source": [
        "##### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syspuJzRAylb"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "PCA_l2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local2_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_l2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL3E4xYWAylb"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "SparsePCA_l2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l2_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La-RPkRxAylc"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "ICA_l2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l2_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l2_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l2_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_l2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8DdPE2oBFPB"
      },
      "source": [
        "##### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DpgyOIIBFPR"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amount\n",
        "PCA_l3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local3_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local3_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local3_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local3_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local3_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_l3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F1jurzwBFPR"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amoun\n",
        "SparsePCA_l3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l3_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN-2H9LwBFPR"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amoun\n",
        "ICA_l3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l3_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l3_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l3_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_l3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvonvzFeBVet"
      },
      "source": [
        "##### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2_opSTUBVe6"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amoun\n",
        "PCA_l4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local4_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local4_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local4_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local4_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local4_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_l4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ove2FSV_BVe7"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amount\n",
        "SparsePCA_l4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l4_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS-BMMOmBVe7"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amount\n",
        "ICA_l4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l4_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l4_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l4_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_l4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUDig1isBVe8"
      },
      "source": [
        "##### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M-0FeDMBVe8"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amoun\n",
        "PCA_l5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_local5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_local5_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_local5_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_local5_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_local5_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_local5_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_l5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qfzuPFLBVe9"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amount\n",
        "SparsePCA_l5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l5_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkGQmjgpBVe-"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amount\n",
        "ICA_l5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_l5_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_l5_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_l5_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_l5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BamdHnecB5gj"
      },
      "source": [
        "#### Original, Local, and Layer-Wise Pruned Compressed (PCA, SparsePCA, ICA) Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AuN1O1oB5hC"
      },
      "source": [
        "##### Pruning Amount = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU96DhXDB5hD"
      },
      "outputs": [],
      "source": [
        "# listing models that have 0.6 pruning amount\n",
        "PCA_sparse1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse1_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse1_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse1_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse1_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse1_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse1_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_sparse1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhNxe5GHB5hD"
      },
      "outputs": [],
      "source": [
        "# listing models that have 0.6 pruning amount\n",
        "SparsePCA_sparse1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse1_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse1_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse1_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_sparse1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1LdVUZwB5hD"
      },
      "outputs": [],
      "source": [
        "# listing models that have 0.6 pruning amount\n",
        "ICA_sparse1_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse1_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse1_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse1_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_sparse1_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXIlvdyIB5hE"
      },
      "source": [
        "##### Pruning Amount = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JyJe-i9NB5hE"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "PCA_sparse2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse2_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse2_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse2_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse2_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse2_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse2_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_sparse2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XwSKO5YTB5hE"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "SparsePCA_sparse2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse2_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse2_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse2_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_sparse2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OzrMv15CB5hG"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.7 pruning amount\n",
        "ICA_sparse2_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse2_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse2_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse2_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_sparse2_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCjkIMsMB5hJ"
      },
      "source": [
        "##### Pruning Amount = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aAldIeVmB5hK"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amount\n",
        "PCA_sparse3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse3_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse3_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse3_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse3_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse3_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse3_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_sparse3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eRGz5FzfB5hL"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amount\n",
        "SparsePCA_sparse3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse3_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse3_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse3_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_sparse3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A6ba95BYB5hL"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.8 pruning amount\n",
        "ICA_sparse3_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse3_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse3_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse3_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_sparse3_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j38gcDrlB5hL"
      },
      "source": [
        "##### Pruning Amount = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QPcMx9BMB5hM"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amount\n",
        "PCA_sparse4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse4_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse4_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse4_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse4_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse4_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse4_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_sparse4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vBWxLBzBB5hM"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amount\n",
        "SparsePCA_sparse4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_sparse4_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_sparse4_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_sparse4_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_sparse4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hLaeb8bPB5hN"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.9 pruning amount\n",
        "ICA_sparse4_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse4_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse4_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse4_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_sparse4_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_oQ7UM-B5hN"
      },
      "source": [
        "##### Pruning Amount = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nWKQeTu4B5hN"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amount\n",
        "PCA_sparse5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 30\": small_5030,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 10\": small_5010,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 5\": small_505,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 10\": small_3010,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 5\": small_305,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 5\": small_105,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 30\": PCA_sparse5_5030,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 10\": PCA_sparse5_5010,\n",
        "    \"PCA Local Compressed - fc1_out = 50 and fc2_out = 5\": PCA_sparse5_505,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 10\": PCA_sparse5_3010,\n",
        "    \"PCA Local Compressed - fc1_out = 30 and fc2_out = 5\": PCA_sparse5_305,\n",
        "    \"PCA Local Compressed - fc1_out = 10 and fc2_out = 5\": PCA_sparse5_105,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in PCA_sparse5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KKikKafAB5hO"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amount\n",
        "SparsePCA_l5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 50 and fc2_out = 32\": SparsePCA_l5_50,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 30 and fc2_out = 32\": SparsePCA_l5_30,\n",
        "    \"SparsePCA Local Compressed - fc1_out = 10 and fc2_out = 32\": SparsePCA_l5_10,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in SparsePCA_l5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eKV_Q_yaB5hO"
      },
      "outputs": [],
      "source": [
        "# listing local models that have 0.95 pruning amount\n",
        "ICA_sparse5_models = {\n",
        "    \"Original\": model,\n",
        "    \"Smaller Network - fc1_out = 50 and fc2_out = 32\": small_5032,\n",
        "    \"Smaller Network - fc1_out = 30 and fc2_out = 32\": small_3032,\n",
        "    \"Smaller Network - fc1_out = 10 and fc2_out = 32\": small_1032,\n",
        "    \"ICA Local Model - fc1_out = 50 and fc2_out = 32\": ICA_sparse5_50,\n",
        "    \"ICA Local Compressed - fc1_out = 30 and fc2_out = 32\": ICA_sparse5_30,\n",
        "    \"ICA Local Compressed - fc1_out = 10 and fc2_out = 32\": ICA_sparse5_10,\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#plot predictions\n",
        "min_val, max_val = float('inf'), -float('inf')\n",
        "\n",
        "for model_name, model in ICA_sparse5_models.items():\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Store actual and predicted values for plotting\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    # Update min and max values for the perfect prediction line\n",
        "    max_val = max(max_val, max(actuals), max(predictions))\n",
        "    min_val = min(min_val, min(actuals), min(predictions))\n",
        "\n",
        "    # Plot actuals vs predictions for this model\n",
        "    plt.scatter(actuals, predictions, alpha=0.5, label=model_name)\n",
        "\n",
        "# perfect prediction line\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2,\n",
        "         label='Perfect Prediction')\n",
        "\n",
        "plt.title('Actual vs Predicted - All Models')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True,\n",
        "           shadow=True, ncol=3)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}