{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyOyfXAI2w9w7gFKQO7auMU8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipKQdxUJrh-l","executionInfo":{"status":"ok","timestamp":1702195818124,"user_tz":0,"elapsed":38405,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"d14fc6d5-9ff9-414f-ae2d-398a5e7224b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#mount to google drive account\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["#import required modules\n","import os\n","import re\n","import time\n","import nltk\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","import torch.nn.utils.prune as prune\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# download the NLTK resources needed\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0RMKkbfrwLB","executionInfo":{"status":"ok","timestamp":1702195872311,"user_tz":0,"elapsed":8426,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"f569c94b-897b-423f-af49-496fa54b3f50"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# path to your processed CSV file\n","data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/IP_MSC/IMDb_processed.csv')"],"metadata":{"id":"-mHf9DCQr0eC","executionInfo":{"status":"ok","timestamp":1702195874797,"user_tz":0,"elapsed":2506,"user":{"displayName":"Melany S","userId":"05076739692052721239"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# get all processed reviews\n","reviews = data.preprocessed.values\n","\n","# merge into single variable, separated by whitespaces\n","words = ' '.join(reviews)\n","\n","# obtain list of words\n","words = words.split()\n","\n","# building a vocabulary and creating mappings between words and integers using a Counter\n","# count the occurrences of words.\n","\n","# counting the occurrences of each word\n","counter = Counter(words)\n","\n","# sorting the words by their frequency in descending order\n","vocab = sorted(counter, key=counter.get, reverse=True)\n","\n","# Create a mapping from integer to word\n","int2word = dict(enumerate(vocab, 1))\n","\n","# Add a special token for padding at index 0\n","int2word[0] = '<PAD>'\n","\n","# creating a mapping from word to integer\n","word2int = {word: id for id, word in int2word.items()}\n","\n","# encoding the words in the reviews text\n","# by using the word-to-integer mapping to replace words with their integer indices\n","reviews_encoded = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n","\n","# showing the 10 encoded words for the first 5 reviews\n","for i in range(5):\n","    print(f\"Review {i + 1}: {reviews_encoded[i][:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yG9Oo_oZr-PQ","executionInfo":{"status":"ok","timestamp":1702195877797,"user_tz":0,"elapsed":3011,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"9e387039-3255-4078-dc06-04306d75419f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 50000/50000 [00:01<00:00, 27739.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Review 1: [172, 1023, 873, 70, 3614, 167, 2924, 1, 102, 2]\n","Review 2: [300, 50, 220, 1, 1204, 1535, 17655, 3, 74, 3]\n","Review 3: [84, 300, 26, 983, 10, 788, 1697, 2532, 2, 1067]\n","Review 4: [2314, 121, 50, 217, 3121, 33, 607, 4182, 542, 825]\n","Review 5: [70690, 10390, 1021, 1859, 7387, 2190, 1248, 5, 40, 1]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Padding sequences function - reviews do not have the same length so we will need set a max sequence length\n","def pad_features(reviews, pad_id, seq_length=128):\n","    # Creating a matrix to store the padded features\n","    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n","    # Looping through each review and pad or trim to the specified sequence length\n","    for i, row in enumerate(reviews):\n","        # If the review is longer than seq_length, trim it\n","        features[i, :len(row)] = np.array(row)[:seq_length]\n","    return features\n","\n","# Set the desired sequence length\n","seq_length = 128\n","\n","# Padding the encoded reviews using the pad_features function\n","features = pad_features(reviews_encoded, pad_id=word2int['<PAD>'], seq_length=seq_length)\n","\n","# Assertions to check the dimensions\n","assert len(features) == len(reviews_encoded)\n","assert len(features[0]) == seq_length\n","\n","# Print the first 10 rows and columns for inspection\n","print(features[:10, :10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgzD1_bRsH0-","executionInfo":{"status":"ok","timestamp":1702195878370,"user_tz":0,"elapsed":584,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"fce3abb9-ea87-44d5-d0e9-bc9dcba35485"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  172  1023   873    70  3614   167  2924     1   102     2]\n"," [  300    50   220     1  1204  1535 17655     3    74     3]\n"," [   84   300    26   983    10   788  1697  2532     2  1067]\n"," [ 2314   121    50   217  3121    33   607  4182   542   825]\n"," [70690 10390  1021  1859  7387  2190  1248     5    40     1]\n"," [ 2691     3    10   337     4     2    13 42241     2  2846]\n"," [  178    14     8    17 12322  1823 55638   106  4989   379]\n"," [   27   389     2  1276  3908   147    30  2854     1    30]\n"," [47622   922   330     5   162   722    70     5     1  1242]\n"," [    8   110  2179  5479  1896     8     4     1   107    74]]\n"]}]},{"cell_type":"code","source":["# create numpy array for labels column\n","labels = data.label.to_numpy()"],"metadata":{"id":"aV-wwRMpsLe5","executionInfo":{"status":"ok","timestamp":1702195886290,"user_tz":0,"elapsed":437,"user":{"displayName":"Melany S","userId":"05076739692052721239"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Train-Test split parameters\n","train_size = 0.8  # We will use 80% of the whole data as the training set\n","val_size = 0.2    # We will use 20% of the remaining data as the validation set\n","\n","# Make the training set\n","split_id = int(len(features) * train_size)\n","train_x, remain_x = features[:split_id], features[split_id:]\n","train_y, remain_y = labels[:split_id], labels[split_id:]\n","\n","# Making validation and test sets\n","split_val_id = int(len(remain_x) * val_size)\n","val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n","val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n","\n","# Printing out the shape of the datasets\n","print('Feature Shapes:')\n","print('===============')\n","print('Train set: {}'.format(train_x.shape))\n","print('Validation set: {}'.format(val_x.shape))\n","print('Test set: {}'.format(test_x.shape))\n","\n","# Print the class distribution in each set\n","print(len(train_y[train_y == 0]), len(train_y[train_y == 1]))\n","print(len(val_y[val_y == 0]), len(val_y[val_y == 1]))\n","print(len(test_y[test_y == 0]), len(test_y[test_y == 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDps357zsMtC","executionInfo":{"status":"ok","timestamp":1702195893142,"user_tz":0,"elapsed":837,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"3011040d-fa4e-4353-aad8-1212aa5f3a73"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Shapes:\n","===============\n","Train set: (40000, 128)\n","Validation set: (2000, 128)\n","Test set: (8000, 128)\n","20007 19993\n","1019 981\n","3974 4026\n"]}]},{"cell_type":"code","source":["# Defining the batch size\n","batch_size = 128\n","\n","# Creating tensor datasets\n","trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","validset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","# Creating data loaders\n","trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n","valloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n","testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"],"metadata":{"id":"AyaW1m99sXy2","executionInfo":{"status":"ok","timestamp":1702195922414,"user_tz":0,"elapsed":966,"user":{"displayName":"Melany S","userId":"05076739692052721239"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Check sample batches from the training loader\n","dataiter = iter(trainloader)\n","x, y = next(dataiter)\n","\n","# Print information about the sample batch\n","print('Sample batch size: ', x.size())   # batch_size, seq_length\n","print('Sample batch input: \\n', x)\n","print()\n","print('Sample label size: ', y.size())   # batch_size\n","print('Sample label input: \\n', y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wV8j-RYpsbOV","executionInfo":{"status":"ok","timestamp":1702195926519,"user_tz":0,"elapsed":13,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"35f9e9fe-84df-4fc4-86f3-e2d71254ba08"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample batch size:  torch.Size([128, 128])\n","Sample batch input: \n"," tensor([[ 2132,  8299,  9111,  ...,     0,     0,     0],\n","        [21546,     2,  1526,  ...,   271,  3124,  2709],\n","        [ 1029,     4,  4393,  ...,     0,     0,     0],\n","        ...,\n","        [  213,   277, 46641,  ...,     0,     0,     0],\n","        [ 3336,  1517,   710,  ...,     0,     0,     0],\n","        [ 1144, 16686,    25,  ...,     0,     0,     0]])\n","\n","Sample label size:  torch.Size([128])\n","Sample label input: \n"," tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n","        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n","        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n","        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n","        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","        0, 0, 1, 0, 0, 1, 1, 0])\n"]}]},{"cell_type":"code","source":["torch.cuda.is_available()\n","# ref: https://stackoverflow.com/questions/50560395/how-to-install-cuda-in-google-colab-gpus\n","# Output would be True if Pytorch is using GPU otherwise it would be False."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Prq_b57xMt_","executionInfo":{"status":"ok","timestamp":1702195934907,"user_tz":0,"elapsed":548,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"f1193d78-facd-476d-f20f-2349efcea61c"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# define training device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYmVXYAUsdzA","executionInfo":{"status":"ok","timestamp":1702195940708,"user_tz":0,"elapsed":377,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"382828b0-ca89-4b7a-96ef-e02d17dada4e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["# Model architecture for sentiment analysis using an LSTM-based neural network\n","class SentimentModel(nn.Module):\n","    def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n","        super(SentimentModel, self).__init__()\n","\n","        # Embedding layer to map input tokens into vector representations\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","\n","        # LSTM layer provided by PyTorch library\n","        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n","\n","        # Dropout layer for regularization\n","        self.dropout = nn.Dropout(0.3)\n","\n","        # Linear layer for the final output\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","        # Sigmoid layer as we are performing binary classification\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Convert input features to the 'long' data type\n","        x = x.long()\n","\n","        # Map input tokens to vector representations using the embedding layer\n","        x = self.embedding(x)\n","\n","        # Pass the input forward through the LSTM layer\n","        o, _ = self.lstm(x)\n","\n","        # Retrieve the last output of the sequence\n","        o = o[:, -1, :]\n","\n","        # Apply dropout and pass through the fully connected layer\n","        o = self.dropout(o)\n","        o = self.fc(o)\n","\n","        # Apply sigmoid activation for binary classification\n","        o = self.sigmoid(o)\n","\n","        return o\n","# reference used: https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7"],"metadata":{"id":"eWZU4iEbsf9L","executionInfo":{"status":"ok","timestamp":1702195944864,"user_tz":0,"elapsed":16,"user":{"displayName":"Melany S","userId":"05076739692052721239"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Model hyperparameters\n","# - `vocab_size`: The size of the vocabulary, representing the number of unique tokens in the input data.\n","# - `output_size`: The size of the output, typically 1 for binary classification (positive or negative sentiment).\n","# - `embedding_size`: The dimensionality of the embedding vectors used to represent each token in the input.\n","# - `hidden_size`: The number of hidden units in the LSTM layer, determining the capacity of the model to capture information.\n","# - `n_layers`: The number of layers in the LSTM, allowing the model to learn hierarchical features.\n","# - `dropout`: The dropout rate, a regularization technique applied to prevent overfitting by randomly dropping units during training.\n","vocab_size = len(word2int)  # Assuming `word2int` is a mapping of words to unique integer indices\n","output_size = 1\n","embedding_size = 256\n","hidden_size = 512\n","n_layers = 2\n","dropout = 0.25\n","\n","# Model initialization\n","# - Create an instance of the SentimentModel class with the specified hyperparameters.\n","# - This initializes the neural network with the defined architecture and sets the hyperparameters.\n","model = SentimentModel(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIpia0Lesgzf","executionInfo":{"status":"ok","timestamp":1702195950782,"user_tz":0,"elapsed":1071,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"efb24e87-3691-420e-a031-f3fd7ded7751"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentModel(\n","  (embedding): Embedding(120982, 256)\n","  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=512, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"]}]},{"cell_type":"code","source":["# Training configuration\n","lr = 0.001\n","criterion = nn.BCELoss()\n","optim = Adam(model.parameters(), lr=lr)\n","grad_clip = 5\n","epochs = 25\n","print_every = 1\n","history = {'train_loss': [],\n","           'train_acc': [],\n","           'val_loss': [],\n","           'val_acc': [],\n","           'epochs': epochs}\n","es_limit = 5\n","\n","# Train loop\n","model = model.to(device)\n","epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n","\n","# Early stop trigger\n","es_trigger = 0\n","val_loss_min = np.Inf\n","\n","# Training and Validation Loop\n","\n","for e in epochloop:\n","\n","    # training mode\n","\n","    # Set the model to training mode to enable gradient calculation\n","    model.train()\n","\n","    train_loss = 0\n","    train_acc = 0\n","\n","    # Iterate over training batches\n","    for id, (feature, target) in enumerate(trainloader):\n","        # Add epoch meta info\n","        epochloop.set_postfix_str(f'Training batch {id}/{len(trainloader)}')\n","\n","        # Move data to the device\n","        feature, target = feature.to(device), target.to(device)\n","\n","        # Reset optimizer\n","        optim.zero_grad()\n","\n","        # Forward pass\n","        out = model(feature)\n","\n","        # Calculate accuracy\n","        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n","        equals = predicted == target\n","        acc = torch.mean(equals.type(torch.FloatTensor))\n","        train_acc += acc.item()\n","\n","        # Calculate loss and perform backpropagation\n","        loss = criterion(out.squeeze(), target.float())\n","        train_loss += loss.item()\n","        loss.backward()\n","\n","        # Clip gradients to prevent exploding gradients\n","        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","        # Update optimizer\n","        optim.step()\n","\n","        # Free some memory\n","        del feature, target, predicted\n","\n","    # Store training metrics\n","    history['train_loss'].append(train_loss / len(trainloader))\n","    history['train_acc'].append(train_acc / len(trainloader))\n","\n","\n","    # validation mode\n","\n","    # Set the model to evaluation mode to disable gradient calculation\n","    model.eval()\n","\n","    val_loss = 0\n","    val_acc = 0\n","\n","    with torch.no_grad():\n","        # Iterate over validation batches\n","        for id, (feature, target) in enumerate(valloader):\n","            # Add epoch meta info\n","            epochloop.set_postfix_str(f'Validation batch {id}/{len(valloader)}')\n","\n","            # Move data to the device\n","            feature, target = feature.to(device), target.to(device)\n","\n","            # Forward pass\n","            out = model(feature)\n","\n","            # Calculate accuracy\n","            predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n","            equals = predicted == target\n","            acc = torch.mean(equals.type(torch.FloatTensor))\n","            val_acc += acc.item()\n","\n","            # Calculate loss\n","            loss = criterion(out.squeeze(), target.float())\n","            val_loss += loss.item()\n","\n","            # Free some memory\n","            del feature, target, predicted\n","\n","        # Store validation metrics\n","        history['val_loss'].append(val_loss / len(valloader))\n","        history['val_acc'].append(val_acc / len(valloader))\n","\n","    # Reset model to training mode\n","    model.train()\n","\n","    # Add epoch meta info\n","    epochloop.set_postfix_str(f'Val Loss: {val_loss / len(valloader):.3f} | Val Acc: {val_acc / len(valloader):.3f}')\n","\n","    # Print epoch information\n","    if (e+1) % print_every == 0:\n","        epochloop.write(f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} Train Acc: {train_acc / len(trainloader):.3f} | Val Loss: {val_loss / len(valloader):.3f} Val Acc: {val_acc / len(valloader):.3f}')\n","        epochloop.update()\n","\n","    # Save model if validation loss decreases\n","    if val_loss / len(valloader) <= val_loss_min:\n","        torch.save(model.state_dict(), './sentiment_lstm.pt')\n","        val_loss_min = val_loss / len(valloader)\n","        es_trigger = 0\n","    else:\n","        epochloop.write(f'[WARNING] Validation loss did not improve ({val_loss_min:.3f} --> {val_loss / len(valloader):.3f})')\n","        es_trigger += 1\n","\n","    # Force early stop\n","    if es_trigger >= es_limit:\n","        epochloop.write(f'Early stopped at Epoch-{e+1}')\n","        # Update epochs history\n","        history['epochs'] = e+1\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bH3BoKkysncg","executionInfo":{"status":"ok","timestamp":1702196144823,"user_tz":0,"elapsed":183332,"user":{"displayName":"Melany S","userId":"05076739692052721239"}},"outputId":"e4a952b8-3eb3-406d-e8ad-d1042ea1f8e8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Training:   4%|▍         | 1/25 [00:18<07:14, 18.11s/it, Val Loss: 0.689 | Val Acc: 0.562]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 | Train Loss: 0.692 Train Acc: 0.525 | Val Loss: 0.689 Val Acc: 0.562\n"]},{"output_type":"stream","name":"stderr","text":["Training:  12%|█▏        | 3/25 [00:35<04:20, 11.83s/it, Val Loss: 0.687 | Val Acc: 0.524]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 | Train Loss: 0.686 Train Acc: 0.556 | Val Loss: 0.687 Val Acc: 0.524\n"]},{"output_type":"stream","name":"stderr","text":["Training:  16%|█▌        | 4/25 [00:52<04:52, 13.95s/it, Val Loss: 0.475 | Val Acc: 0.782]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 | Train Loss: 0.595 Train Acc: 0.661 | Val Loss: 0.475 Val Acc: 0.782\n"]},{"output_type":"stream","name":"stderr","text":["Training:  24%|██▍       | 6/25 [01:09<03:42, 11.71s/it, Val Loss: 0.378 | Val Acc: 0.840]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 | Train Loss: 0.360 Train Acc: 0.850 | Val Loss: 0.378 Val Acc: 0.840\n"]},{"output_type":"stream","name":"stderr","text":["Training:  28%|██▊       | 7/25 [01:26<04:03, 13.52s/it, Val Loss: 0.349 | Val Acc: 0.853]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 | Train Loss: 0.248 Train Acc: 0.906 | Val Loss: 0.349 Val Acc: 0.853\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▌      | 9/25 [01:44<03:55, 14.71s/it, Training batch 3/313]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 | Train Loss: 0.176 Train Acc: 0.938 | Val Loss: 0.370 Val Acc: 0.855\n","[WARNING] Validation loss did not improve (0.349 --> 0.370)\n"]},{"output_type":"stream","name":"stderr","text":["Training:  40%|████      | 10/25 [02:01<02:56, 11.74s/it, Training batch 3/313]"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 | Train Loss: 0.120 Train Acc: 0.960 | Val Loss: 0.449 Val Acc: 0.848\n","[WARNING] Validation loss did not improve (0.349 --> 0.449)\n"]},{"output_type":"stream","name":"stderr","text":["Training:  44%|████▍     | 11/25 [02:18<03:02, 13.07s/it, Training batch 3/313]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 | Train Loss: 0.074 Train Acc: 0.977 | Val Loss: 0.495 Val Acc: 0.849\n","[WARNING] Validation loss did not improve (0.349 --> 0.495)\n"]},{"output_type":"stream","name":"stderr","text":["Training:  52%|█████▏    | 13/25 [02:35<02:49, 14.11s/it, Training batch 3/313]"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 | Train Loss: 0.050 Train Acc: 0.987 | Val Loss: 0.575 Val Acc: 0.857\n","[WARNING] Validation loss did not improve (0.349 --> 0.575)\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▌      | 9/25 [02:51<05:05, 19.11s/it, Val Loss: 0.635 | Val Acc: 0.854] "]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25 | Train Loss: 0.036 Train Acc: 0.991 | Val Loss: 0.635 Val Acc: 0.854\n","[WARNING] Validation loss did not improve (0.349 --> 0.635)\n","Early stopped at Epoch-10\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zKN_omdLyRqM"},"execution_count":null,"outputs":[]}]}