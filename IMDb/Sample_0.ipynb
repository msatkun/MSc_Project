{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b749db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9002447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 50000/50000 [00:00<00:00, 350879.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1\n",
       "2  I thought this was a wonderful way to spend ti...  positive      1\n",
       "3  Basically there's a family where a little boy ...  negative      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import csv file using pandas\n",
    "df = pd.read_csv('IMDB Dataset.csv', encoding='utf-8')\n",
    "\n",
    "#defined a transformation fn to change negative values as zero and positive as one\n",
    "def transform_lbl(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "# progress bar for pandas functions\n",
    "# reference: https://www.kdnuggets.com/2022/09/progress-bars-python-tqdm-fun-profit.html\n",
    "# https://towardsdatascience.com/progress-bars-in-python-and-pandas-f81954d33bae\n",
    "tqdm.pandas()\n",
    "\n",
    "# apply the transformation fn on the IMDb dataset\n",
    "df['label'] = df['sentiment'].progress_apply(transform_lbl)\n",
    "\n",
    "# check if the changes have been applied\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e821a3",
   "metadata": {},
   "source": [
    "## Cleaning up the Dataset\n",
    "Need to remove the following things: \n",
    "- HTML Marks including square brackets in the text\n",
    "- Contracted Words\n",
    "- Extra White Space\n",
    "- Stemming Words\n",
    "- Stopwords\n",
    "\n",
    "References used: \n",
    "\n",
    "https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
    "\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "\n",
    "https://lzone.de/examples/Python%20re.sub\n",
    "\n",
    "https://medium.com/@yashj302/text-cleaning-using-regex-python-f1dded1ac5bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a78f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/smelany/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/smelany/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/smelany/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/smelany/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the NLTK resources needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4db890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your stopwords set\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# removing hyperlinks and URLs\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "# removing HTML tags\n",
    "def rm_html(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# removing regular punctuations\n",
    "def rm_punct2(text):\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "# add spacing between punctations marks\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)\n",
    "    return s\n",
    "\n",
    "# remove number \n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# remove any additional white spaces\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "# remove NONASCII characters\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "# remove emojis from text\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'\n",
    "        u'\\U0001F300-\\U0001F5FF'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\U0001F1E0-\\U0001F1FF'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "# removes repeated characters - e.g. 'heeellllooo' will be 'hello'\n",
    "def spell_correction(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "#tokenize\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "#removing stopwords\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i.lower() not in stopwords_set]\n",
    "\n",
    "# lemmatize text\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "# function to cleanup the text\n",
    "def cleanup(text):\n",
    "    no_link = rm_link(text)\n",
    "    no_html = rm_html(no_link)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    spell_corrected = spell_correction(no_emoji)\n",
    "    return spell_corrected\n",
    "\n",
    "# function to preprocess the text\n",
    "def preprocess_pipeline(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2487b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [00:29<00:00, 1668.89it/s]\n",
      "100%|████████████████████████████████████| 50000/50000 [02:46<00:00, 300.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One reviewer mentioned watching Oz episode hoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production . The filming te...</td>\n",
       "      <td>wonderful little production . filming techniqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there s a family where a little boy ...</td>\n",
       "      <td>Basically family little boy Jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei s Love in the Time of Money is a...</td>\n",
       "      <td>Petter Mattei Love Time Money visually stunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive      1   \n",
       "1  A wonderful little production. <br /><br />The...  positive      1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive      1   \n",
       "3  Basically there's a family where a little boy ...  negative      0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1   \n",
       "\n",
       "                                               clean  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production . The filming te...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there s a family where a little boy ...   \n",
       "4  Petter Mattei s Love in the Time of Money is a...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  One reviewer mentioned watching Oz episode hoo...  \n",
       "1  wonderful little production . filming techniqu...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  Basically family little boy Jake think zombie ...  \n",
       "4  Petter Mattei Love Time Money visually stunnin...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply cleanup function on the text\n",
    "df['clean'] = df['review'].progress_apply(cleanup)\n",
    "\n",
    "#apply \n",
    "df['preprocessed'] = df['clean'].progress_apply(preprocess_pipeline)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5c9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new dataframe with preprocessed text as a new csv file\n",
    "df.to_csv('IMDb_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d8bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focusing only on the processed text and the labels\n",
    "# keep only processed and label columns\n",
    "df[['preprocessed', 'label']].to_csv('./imdb_0.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee5e1f",
   "metadata": {},
   "source": [
    "Padding and Mapping of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35441333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One reviewer mentioned watching Oz episode hooked . right , exactly happened . first thing struck Oz brutality unflinching scene violence , set right word GO . Trust , show faint hearted timid . show pull punch regard drug , sex violence . hardcore , classic use word . called OZ nickname given Oswald Maximum Security State Penitentary . focus mainly Emerald City , experimental section prison cell glass front face inwards , privacy high agenda . Em City home many . . Aryans , Muslims , gangsta , Latinos , Christians , Italians , Irish . . . . scuffle , death stare , dodgy dealing shady agreement never far away . would say main appeal show due fact go show dare . Forget pretty picture painted mainstream audience , forget charm , forget romance . . . OZ mess around . first episode ever saw struck nasty surreal , say ready , watched , developed taste Oz , got accustomed high level graphic violence . violence , injustice crooked guard sold nickel , inmate kill order get away , well mannered , middle class inmate turned prison bitch due lack street skill prison experience Watching Oz , may become comfortable uncomfortable viewing . . . . thats get touch darker side .\n",
      "Label: 1\n",
      "\n",
      "\n",
      "wonderful little production . filming technique unassuming - old - time - BBC fashion give comforting , sometimes discomforting , sense realism entire piece . actor extremely well chosen - Michael Sheen got polari voice pat ! truly see seamless editing guided reference Williams diary entry , well worth watching terrificly written performed piece . masterful production one great master comedy life . realism really come home little thing fantasy guard , rather use traditional dream technique remains solid disappears . play knowledge sens , particularly scene concerning Orton Halliwell set particularly flat Halliwell mural decorating every surface terribly well done .\n",
      "Label: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path to your processed CSV file\n",
    "data = pd.read_csv('./imdb_0.csv')\n",
    "\n",
    "# Iterating over the first two rows of the DataFrame\n",
    "for row in data[:2].iterrows():\n",
    "    # Printing the 'processed' column\n",
    "    print(row[1]['preprocessed'])\n",
    "    \n",
    "    # Printing the 'label' column\n",
    "    print(f'Label: {row[1][\"label\"]}')\n",
    "    \n",
    "    # Printing a newline for better readability between rows\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb2267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all processed reviews\n",
    "reviews = data.preprocessed.values\n",
    "\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "\n",
    "# obtain list of words\n",
    "words = words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc2c5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One', 'reviewer', 'mentioned', 'watching', 'Oz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check list\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b24895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a vocabulary and creating mappings between words and integers using a Counter \n",
    "# count the occurrences of words.\n",
    "\n",
    "# counting the occurrences of each word\n",
    "counter = Counter(words)\n",
    "\n",
    "# sorting the words by their frequency in descending order\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "\n",
    "# Create a mapping from integer to word\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "\n",
    "# Add a special token for padding at index 0\n",
    "int2word[0] = '<PAD>'\n",
    "\n",
    "# creating a mapping from word to integer\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d13ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50000/50000 [00:02<00:00, 18577.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# encoding the words in the reviews text\n",
    "# by using the word-to-integer mapping to replace words with their integer indices\n",
    "reviews_encoded = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70fd8269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: [172, 1023, 873, 70, 3614, 167, 2924, 1, 102, 2]\n",
      "Review 2: [300, 50, 220, 1, 1204, 1535, 17655, 3, 74, 3]\n",
      "Review 3: [84, 300, 26, 983, 10, 788, 1697, 2532, 2, 1067]\n",
      "Review 4: [2314, 121, 50, 217, 3121, 33, 607, 4182, 542, 825]\n",
      "Review 5: [70690, 10390, 1021, 1859, 7387, 2190, 1248, 5, 40, 1]\n"
     ]
    }
   ],
   "source": [
    "# showing the 10 encoded words for the first 5 reviews \n",
    "for i in range(5):\n",
    "    print(f\"Review {i + 1}: {reviews_encoded[i][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f476c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences function - reviews do not have the same length so we will need set a max sequence length\n",
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    # Creating a matrix to store the padded features\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "    # Looping through each review and pad or trim to the specified sequence length\n",
    "    for i, row in enumerate(reviews):\n",
    "        # If the review is longer than seq_length, trim it\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "\n",
    "# Set the desired sequence length\n",
    "seq_length = 256\n",
    "\n",
    "# Padding the encoded reviews using the pad_features function\n",
    "features = pad_features(reviews_encoded, pad_id=word2int['<PAD>'], seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd006a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  172  1023   873    70  3614   167  2924     1   102     2]\n",
      " [  300    50   220     1  1204  1535 17655     3    74     3]\n",
      " [   84   300    26   983    10   788  1697  2532     2  1067]\n",
      " [ 2314   121    50   217  3121    33   607  4182   542   825]\n",
      " [70690 10390  1021  1859  7387  2190  1248     5    40     1]\n",
      " [ 2691     3    10   337     4     2    13 42241     2  2846]\n",
      " [  178    14     8    17 12322  1823 55638   106  4989   379]\n",
      " [   27   389     2  1276  3908   147    30  2854     1    30]\n",
      " [47622   922   330     5   162   722    70     5     1  1242]\n",
      " [    8   110  2179  5479  1896     8     4     1   107    74]]\n"
     ]
    }
   ],
   "source": [
    "# Assertions to check the dimensions\n",
    "assert len(features) == len(reviews_encoded)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "# Print the first 10 rows and columns for inspection\n",
    "print(features[:10, :10])\n",
    "\n",
    "# create numpy array for labels column\n",
    "labels = data.label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc34389",
   "metadata": {},
   "source": [
    "Training and Testing Set\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c21552df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (40000, 256)\n",
      "Validation set: (5000, 256)\n",
      "Test set: (5000, 256)\n",
      "20007 19993\n",
      "2463 2537\n",
      "2530 2470\n"
     ]
    }
   ],
   "source": [
    "# Train-Test split parameters\n",
    "train_size = 0.8  # We will use 80% of the whole data as the training set\n",
    "val_size = 0.5    # We will use 50% of the remaining data as the validation set\n",
    "\n",
    "# Make the training set\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "# Making validation and test sets\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "\n",
    "# Printing out the shape of the datasets\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))\n",
    "\n",
    "# Print the class distribution in each set\n",
    "print(len(train_y[train_y == 0]), len(train_y[train_y == 1]))\n",
    "print(len(val_y[val_y == 0]), len(val_y[val_y == 1]))\n",
    "print(len(test_y[test_y == 0]), len(test_y[test_y == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b7c2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Creating tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "validset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# Creating data loaders\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
    "testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ef0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch size:  torch.Size([128, 256])\n",
      "Sample batch input: \n",
      " tensor([[11192,  5667,     2,  ...,   555,   179,    70],\n",
      "        [ 1479,     1,  1998,  ...,     0,     0,     0],\n",
      "        [  500,     2,    30,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1609,    55,    10,  ..., 26205,     1,  4391],\n",
      "        [ 2270,    17,     4,  ...,     0,     0,     0],\n",
      "        [  569,   869,  2060,  ...,     1,    30,  2557]])\n",
      "\n",
      "Sample label size:  torch.Size([128])\n",
      "Sample label input: \n",
      " tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Check sample batches from the training loader\n",
    "dataiter = iter(trainloader)\n",
    "x, y = dataiter.next()\n",
    "\n",
    "# Print information about the sample batch\n",
    "print('Sample batch size: ', x.size())   # batch_size, seq_length\n",
    "print('Sample batch input: \\n', x)\n",
    "print()\n",
    "print('Sample label size: ', y.size())   # batch_size\n",
    "print('Sample label input: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981904d",
   "metadata": {},
   "source": [
    "# Apply Pruning\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#sphx-glr-intermediate-pruning-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cfa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09aec8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cfd2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture for sentiment analysis using an LSTM-based neural network\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        # Embedding layer to map input tokens into vector representations\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM layer provided by PyTorch library\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Linear layer for the final output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Sigmoid layer as we are performing binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input features to the 'long' data type\n",
    "        x = x.long()\n",
    "\n",
    "        # Map input tokens to vector representations using the embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass the input forward through the LSTM layer\n",
    "        o, _ = self.lstm(x)\n",
    "\n",
    "        # Retrieve the last output of the sequence\n",
    "        o = o[:, -1, :]\n",
    "\n",
    "        # Apply dropout and pass through the fully connected layer\n",
    "        o = self.dropout(o)\n",
    "        o = self.fc(o)\n",
    "\n",
    "        # Apply sigmoid activation for binary classification\n",
    "        o = self.sigmoid(o)\n",
    "\n",
    "        return o\n",
    "# reference used: https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1570ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentModel(\n",
      "  (embedding): Embedding(120982, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "# - `vocab_size`: The size of the vocabulary, representing the number of unique tokens in the input data.\n",
    "# - `output_size`: The size of the output, typically 1 for binary classification (positive or negative sentiment).\n",
    "# - `embedding_size`: The dimensionality of the embedding vectors used to represent each token in the input.\n",
    "# - `hidden_size`: The number of hidden units in the LSTM layer, determining the capacity of the model to capture information.\n",
    "# - `n_layers`: The number of layers in the LSTM, allowing the model to learn hierarchical features.\n",
    "# - `dropout`: The dropout rate, a regularization technique applied to prevent overfitting by randomly dropping units during training.\n",
    "vocab_size = len(word2int)  # Assuming `word2int` is a mapping of words to unique integer indices\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout = 0.25\n",
    "\n",
    "# Model initialization\n",
    "# - Create an instance of the SentimentModel class with the specified hyperparameters.\n",
    "# - This initializes the neural network with the defined architecture and sets the hyperparameters.\n",
    "model = SentimentModel(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00312650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1, bias=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply weight pruning to the Linear layer (fc) of the model\n",
    "prune.l1_unstructured(model.fc, name='weight', amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "152ac07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc9ad192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "grad_clip = 5\n",
    "epochs = 25\n",
    "print_every = 1\n",
    "history = {'train_loss': [], \n",
    "           'train_acc': [], \n",
    "           'val_loss': [], \n",
    "           'val_acc': [], \n",
    "           'epochs': epochs}\n",
    "es_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfec487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                    | 0/25 [43:55<?, ?it/s, Training batch 7/313]"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "\n",
    "# Early stop trigger\n",
    "es_trigger = 0\n",
    "val_loss_min = float('inf')\n",
    "\n",
    "# Training and Validation Loop\n",
    "\n",
    "for e in epochloop:\n",
    "\n",
    "    # training mode\n",
    "\n",
    "    # Set the model to training mode to enable gradient calculation\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    # Iterate over training batches\n",
    "    for id, (feature, target) in enumerate(trainloader):\n",
    "        # Add epoch meta info\n",
    "        epochloop.set_postfix_str(f'Training batch {id}/{len(trainloader)}')\n",
    "\n",
    "        # Move data to the device\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        # Reset optimizer\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(feature)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "\n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update optimizer\n",
    "        optim.step()\n",
    "\n",
    "        # Free some memory\n",
    "        del feature, target, predicted\n",
    "\n",
    "    # Store training metrics\n",
    "    history['train_loss'].append(train_loss / len(trainloader))\n",
    "    history['train_acc'].append(train_acc / len(trainloader))\n",
    "\n",
    "    \n",
    "    # validation mode\n",
    "    \n",
    "    # Set the model to evaluation mode to disable gradient calculation\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over validation batches\n",
    "        for id, (feature, target) in enumerate(valloader):\n",
    "            # Add epoch meta info\n",
    "            epochloop.set_postfix_str(f'Validation batch {id}/{len(valloader)}')\n",
    "            \n",
    "            # Move data to the device\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(feature)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Free some memory\n",
    "            del feature, target, predicted\n",
    "\n",
    "        # Store validation metrics\n",
    "        history['val_loss'].append(val_loss / len(valloader))\n",
    "        history['val_acc'].append(val_acc / len(valloader))\n",
    "\n",
    "    # Reset model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Add epoch meta info\n",
    "    epochloop.set_postfix_str(f'Val Loss: {val_loss / len(valloader):.3f} | Val Acc: {val_acc / len(valloader):.3f}')\n",
    "\n",
    "    # Print epoch information\n",
    "    if (e+1) % print_every == 0:\n",
    "        epochloop.write(f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} Train Acc: {train_acc / len(trainloader):.3f} | Val Loss: {val_loss / len(valloader):.3f} Val Acc: {val_acc / len(valloader):.3f}')\n",
    "        epochloop.update()\n",
    "\n",
    "    # Save model if validation loss decreases\n",
    "    if val_loss / len(valloader) <= val_loss_min:\n",
    "        torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        val_loss_min = val_loss / len(valloader)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        epochloop.write(f'[WARNING] Validation loss did not improve ({val_loss_min:.3f} --> {val_loss / len(valloader):.3f})')\n",
    "        es_trigger += 1\n",
    "\n",
    "    # Force early stop\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "        # Update epochs history\n",
    "        history['epochs'] = e+1\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36c193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
