{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f8abdb",
   "metadata": {},
   "source": [
    "# Individual Project (MSc) - Methods for Compressing Different Types of Neural Networks\n",
    "\n",
    "This file has the full code for the project.\n",
    "\n",
    "## Preparing the Dataset\n",
    "The dataset was obtained from Kaggle, https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount to google drive account\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv file using pandas\n",
    "df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/IP_MSC/IMDB Dataset.csv', encoding='utf-8')\n",
    "\n",
    "#transformation funciton to change negative values as zero and positive as one\n",
    "def lbl_transform(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "# progress bar for pandas functions\n",
    "# reference: https://www.kdnuggets.com/2022/09/progress-bars-python-tqdm-fun-profit.html\n",
    "# https://towardsdatascience.com/progress-bars-in-python-and-pandas-f81954d33bae\n",
    "tqdm.pandas()\n",
    "\n",
    "# apply the transformation function in the Labels column on the IMDb dataset\n",
    "df['label'] = df['sentiment'].progress_apply(lbl_transform)\n",
    "\n",
    "# check if the changes have been applied\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0805c",
   "metadata": {},
   "source": [
    "### Cleaning up the Dataset\n",
    "Need to remove the following things: \n",
    "- HTML Marks including square brackets in the text\n",
    "- Contracted Words\n",
    "- Extra White Space\n",
    "- Stemming Words\n",
    "- Stopwords\n",
    "\n",
    "References used: \n",
    "\n",
    "https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
    "\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "\n",
    "https://lzone.de/examples/Python%20re.sub\n",
    "\n",
    "https://medium.com/@yashj302/text-cleaning-using-regex-python-f1dded1ac5bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e106c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# defining stopwords for English\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing hyperlinks and URLs\n",
    "def remove_links(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "# removing HTML tags in text\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# removing regular punctuations\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "# add spacing between punctations marks\n",
    "def spacing4punctuations(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)\n",
    "    return s\n",
    "\n",
    "# removing any additional white spaces\n",
    "def remove_spacing(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "# removing number in text\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# removing NONASCII characters\n",
    "def remove_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "# removing emojis from text\n",
    "def remove_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'\n",
    "        u'\\U0001F300-\\U0001F5FF'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\U0001F1E0-\\U0001F1FF'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "# removes repeated characters - e.g. 'heeellllooo' will be 'hello'\n",
    "def auto_correct(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "# tokenize the text\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "#removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    return [i for i in text if i.lower() not in stopwords]\n",
    "\n",
    "# lemmatize text\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    return remove_stopwords(lemmas)\n",
    "\n",
    "# function to cleaning up the text with all the function defined above\n",
    "def txt_cleanup(text):\n",
    "    no_link = remove_links(text)\n",
    "    no_html = remove_html(no_link)\n",
    "    space_punct = spacing4punctuations(no_html)\n",
    "    no_punct = remove_punctuations(space_punct)\n",
    "    no_number = remove_numbers(no_punct)\n",
    "    no_whitespaces = remove_spacing(no_number)\n",
    "    no_nonasci = remove_nonascii(no_whitespaces)\n",
    "    no_emoji = remove_emoji(no_nonasci)\n",
    "    spell_corrected = auto_correct(no_emoji)\n",
    "    return spell_corrected\n",
    "\n",
    "# function to preprocess the text\n",
    "def txt_processing(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = remove_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# apply txt_cleanup function on the IMDb dataset\n",
    "df['clean'] = df['review'].progress_apply(txt_cleanup)\n",
    "\n",
    "# applying the text preprocessing function on the IMDb dataset\n",
    "df['preprocessed'] = df['clean'].progress_apply(txt_processing)\n",
    "\n",
    "# view the changes that were made on the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fee11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new dataframe with preprocessed text as a new csv file\n",
    "df.to_csv('IMDb_processed.csv', index=False)\n",
    "\n",
    "# focusing only on the processed text and the labels\n",
    "# keep only processed and label columns\n",
    "df[['preprocessed', 'label']].to_csv('./IMDb_processed.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a0610",
   "metadata": {},
   "source": [
    "## Comprehensive Text Data Processing and Encoding\n",
    "\n",
    "I have created a pipeline for processing the IMDB text data in this section. It prepares IMDB reviews by combining preprocessed reviews, segmenting the text into words, and building a vocabulary. Words are mapped to integers to encode reviews into numerical representations, and a padding token is introduced. The padding ensures that all reviews have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6866e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your processed CSV file\n",
    "data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/IP_MSC/IMDb_processed.csv')\n",
    "\n",
    "# checking the length of the processed reviews\n",
    "data['review_length']=data['preprocessed'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# plotting a bar chart to view the review lengths\n",
    "with plt.style.context(style='bmh'):\n",
    "    data['review_length'].hist(figsize=(15,5))\n",
    "    plt.title(label='Feature Length of Reviews in the IMDb dataset')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995de940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining all processed reviews\n",
    "processed_reviews = data.preprocessed.values\n",
    "\n",
    "# merging into single variable, separated by whitespaces\n",
    "words = ' '.join(processed_reviews)\n",
    "\n",
    "# obtaining a list of words\n",
    "words = words.split()\n",
    "\n",
    "# building a vocabulary and creating mappings between words and integers using a Counter\n",
    "# counting the occurrences of each word\n",
    "counter = Counter(words)\n",
    "\n",
    "# sorting the words by their frequency in descending order\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "\n",
    "# Create a mapping from integer to word\n",
    "convert2integer = dict(enumerate(vocab, 1))\n",
    "\n",
    "# Adding a special token for padding at index 0\n",
    "convert2integer[0] = '<PAD>'\n",
    "\n",
    "# creating a mapping from word to integer\n",
    "convert2word = {word: id for id, word in convert2integer.items()}\n",
    "\n",
    "# encoding the words in the reviews text\n",
    "# by using the word-to-integer mapping to replace words with their integer indices\n",
    "reviews_encoded = [[convert2word[word] for word in review.split()] for review in tqdm(processed_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences function\n",
    "#processed_reviews do not have the same length so we will need set a max sequence length\n",
    "def pad_features(processed_reviews, pad_id, sequence_length=128):\n",
    "    # Creating a matrix to store the padded features\n",
    "    features = np.full((len(processed_reviews), sequence_length), pad_id, dtype=int)\n",
    "    # Looping through each review and pad or trim to the specified sequence length\n",
    "    for i, row in enumerate(processed_reviews):\n",
    "        # If the review is longer than sequence_length, trim it\n",
    "        features[i, :len(row)] = np.array(row)[:sequence_length]\n",
    "    return features\n",
    "\n",
    "# Set the desired sequence length\n",
    "sequence_length = 128\n",
    "\n",
    "# Padding the encoded reviews using the pad_features function\n",
    "features = pad_features(reviews_encoded, pad_id=convert2word['<PAD>'], sequence_length=sequence_length)\n",
    "\n",
    "# Assertions to check the dimensions - verify whether the dimensions of the padded features match the expected values\n",
    "assert len(features) == len(reviews_encoded)\n",
    "assert len(features[0]) == sequence_length\n",
    "\n",
    "# printing the first 10 rows and columns for inspection\n",
    "print(features[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array for labels column\n",
    "labels = data.label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee526d",
   "metadata": {},
   "source": [
    "## Splitting the Dataset to Training and Testing\n",
    "\n",
    "We divide the dataset into three parts - training, validation, and test sets. 70% data is allocated for training and the remaining 30% for validation and testing. We separate the features and labels accordingly, print their shapes, and display the class distribution within each set to understand how the binary classes are balanced. This process is crucial for reliable model development and assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting values for training and testing\n",
    "train_size = 0.7  # use 70% of the data as the training set\n",
    "val_size = 0.3    # use 30% of the remaining data as the validation set\n",
    "\n",
    "# making the training set\n",
    "# calculating the index to split the features and labels for the training set\n",
    "split_id = int(len(features) * train_size)\n",
    "# splitting the features into the training set\n",
    "X_train, X_remainder = features[:split_id], features[split_id:]\n",
    "# splitting the labels into the training set\n",
    "Y_train, Y_remainder = labels[:split_id], labels[split_id:]\n",
    "\n",
    "# making the validation and testing set\n",
    "# calculating the index to split the remaining data into validation and test sets\n",
    "split_val_id = int(len(X_remainder) * val_size)\n",
    "# splitting the remaining features into validation and test sets\n",
    "X_validation, X_test = X_remainder[:split_val_id], X_remainder[split_val_id:]\n",
    "# splitting the remaining labels into validation and test sets\n",
    "Y_validation, Y_test = Y_remainder[:split_val_id], Y_remainder[split_val_id:]\n",
    "\n",
    "# Printing out the shape of the datasets\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "# Print the shape of the training set features\n",
    "print('Training set: {}'.format(X_train.shape)) \n",
    "# Print the shape of the validation set features\n",
    "print('Validation set: {}'.format(X_validation.shape))\n",
    "# Print the shape of the test set features\n",
    "print('Testing set: {}'.format(X_test.shape))\n",
    "\n",
    "# Print the class distribution in each set\n",
    "print(\"Class Distribution in Training Set:\")\n",
    "# Print the count of each class in the training set\n",
    "print(f\"Class 0: {len(Y_train[Y_train == 0])}, Class 1: {len(Y_train[Y_train == 1])}\")\n",
    "# Print the count of each class in the validation set\n",
    "print(\"\\nClass Distribution in Validation Set:\")\n",
    "print(f\"Class 0: {len(Y_validation[Y_validation == 0])}, Class 1: {len(Y_validation[Y_validation == 1])}\")\n",
    "# Print the count of each class in the test set\n",
    "print(\"\\nClass Distribution in Test Set:\")\n",
    "print(f\"Class 0: {len(Y_test[Y_test == 0])}, Class 1: {len(Y_test[Y_test == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f219ac7",
   "metadata": {},
   "source": [
    "Now, we have a **batch size of 64** for efficient training and three PyTorch TensorDatasets: **training_set**, **validation_set**, and **testing_set**. Three DataLoader instances initialize with batch size and enable shuffling for training and validation sets. \n",
    "\n",
    "The code checks a sample batch from the training loader to provide insights into the  structure of the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe30721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Creating tensor datasets\n",
    "training_set = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train))\n",
    "validation_set = TensorDataset(torch.from_numpy(X_validation), torch.from_numpy(Y_validation))\n",
    "testing_set = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test))\n",
    "\n",
    "# Creating data loaders\n",
    "training_loader = DataLoader(training_set, shuffle=True, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_set, shuffle=True, batch_size=batch_size)\n",
    "testing_loader = DataLoader(testing_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Checking sample batches from the training loader\n",
    "dataiteration = iter(training_loader)\n",
    "x, y = next(dataiteration)\n",
    "\n",
    "# Display details of the sample batch\n",
    "print('Sample Batch Information:')\n",
    "print('-------------------------')\n",
    "print('Input Size: ', x.size())   # Display the size of the input batch (batch_size, seq_length)\n",
    "print('Input Batch:\\n', x)        # Display the input batch\n",
    "print()\n",
    "print('Label Size: ', y.size())   # Display the size of the label batch (batch_size)\n",
    "print('Label Batch:\\n', y)        # Display the label batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540f88b",
   "metadata": {},
   "source": [
    "## LSTM-based model for Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ca50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "# ref: https://stackoverflow.com/questions/50560395/how-to-install-cuda-in-google-colab-gpus\n",
    "# Output would be True if Pytorch is using GPU otherwise it would be False.\n",
    "\n",
    "# define training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model architecture for sentiment analysis\n",
    "# reference used: https://galhever.medium.com/sentiment-analysis-with-pytorch-part-3-cnn-model-7bb30712abd7\n",
    "class Sentiment_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        super(Sentiment_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding layer to map input tokens into vector representations\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM layer provided by PyTorch library\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Linear layer for the final output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Sigmoid layer as we are performing binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input features to the 'long' data type\n",
    "        x = x.long()\n",
    "\n",
    "        # Map input tokens to vector representations using the embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass the input forward through the LSTM layer\n",
    "        o, _ = self.lstm(x)\n",
    "\n",
    "        # Retrieve the last output of the sequence\n",
    "        o = o[:, -1, :]\n",
    "\n",
    "        # Apply dropout and pass through the fully connected layer\n",
    "        o = self.dropout(o)\n",
    "        o = self.fc(o)\n",
    "\n",
    "        # Apply sigmoid activation for binary classification\n",
    "        o = self.sigmoid(o)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(convert2word)  # size of the vocabular; assuming `convert2word` is a mapping of words to unique integer indices\n",
    "output_size = 1 # size of the output, typically 1 for binary classification (positive or negative sentiment)\n",
    "embedding_size = 256 #dimensionality of embedding vectors used to represent each token in the input\n",
    "hidden_size = 512 #number of hidden units in the LSTM layer\n",
    "n_layers = 2 #number of layers in the LSTM model\n",
    "dropout = 0.25 #dropout rate; regularization technique applied to prevent overfitting by randomly dropping units during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = Sentiment_LSTM(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "lr = 0.001  # Learning rate\n",
    "criterion = nn.BCELoss()  # Loss function: Binary Cross Entropy Loss for binary classification\n",
    "optim = Adam(model.parameters(), lr=lr)  # Optimizer: Adam optimizer with the specified learning rate\n",
    "grad_clip = 5  # Gradient clipping threshold to prevent large gradients\n",
    "epochs = 10  # Number of times the entire dataset is passed through the model during training\n",
    "print_every = 1  # Frequency of printing training progress information (every 'print_every' epochs)\n",
    "history = {\n",
    "    'training_loss': [],          # List to store training loss over epochs\n",
    "    'training_accuracy': [],      # List to store training accuracy over epochs\n",
    "    'validation_loss': [],        # List to store validation loss over epochs\n",
    "    'validation_accuracy': [],    # List to store validation accuracy over epochs\n",
    "    'epochs': epochs              # Total number of epochs\n",
    "}\n",
    "es_limit = 5  # Early stopping limit; Maximum consecutive epochs without improvement in validation loss\n",
    "\n",
    "# Training loop\n",
    "\n",
    "# Move the model to the specified device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a tqdm progress bar over the range of epochs\n",
    "# - 'position=0': Display the progress bar at the top\n",
    "# - 'desc='Training'': Description to display in the progress bar\n",
    "# - 'leave=True': Leave the progress bar displayed after completion\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "\n",
    "# Early stop trigger\n",
    "es_trigger = 0\n",
    "\n",
    "# Minimum validation loss initialized to positive infinity\n",
    "# Used to track the lowest validation loss encountered during training\n",
    "validation_loss_min = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loop\n",
    "# reference used: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "for e in epochloop:\n",
    "\n",
    "    # Training mode\n",
    "    # Set the model to training mode to enable gradient calculation\n",
    "    model.train()\n",
    "\n",
    "    # Initialize training metrics\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "\n",
    "    # Iterate over training batches\n",
    "    for id, (feature, target) in enumerate(training_loader):\n",
    "        # Add epoch meta info\n",
    "        epochloop.set_postfix_str(f'Training batch {id}/{len(training_loader)}')\n",
    "\n",
    "        # Move data to the specified device (GPU or CPU)\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        # Reset optimizer gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(feature)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        training_accuracy += acc.item()\n",
    "\n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update optimizer\n",
    "        optim.step()\n",
    "\n",
    "        del feature, target, predicted\n",
    "\n",
    "    # Store training metrics\n",
    "    history['training_loss'].append(training_loss / len(training_loader))\n",
    "    history['training_accuracy'].append(training_accuracy / len(training_loader))\n",
    "\n",
    "    # Validation mode\n",
    "    # Set the model to evaluation mode to disable gradient calculation\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation metrics\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over validation batches\n",
    "        for id, (feature, target) in enumerate(validation_loader):\n",
    "            # Add epoch meta info\n",
    "            epochloop.set_postfix_str(f'Validation batch {id}/{len(validation_loader)}')\n",
    "\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(feature)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            validation_accuracy += acc.item()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Free some memory\n",
    "            del feature, target, predicted\n",
    "\n",
    "        # Store validation metrics\n",
    "        history['validation_loss'].append(validation_loss / len(validation_loader))\n",
    "        history['validation_accuracy'].append(validation_accuracy / len(validation_loader))\n",
    "\n",
    "    # Reset model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Add epoch meta info\n",
    "    epochloop.set_postfix_str(f'Validation Loss: {validation_loss / len(validation_loader):.3f} | Validation Accuracy: {validation_accuracy / len(validation_loader):.3f}')\n",
    "\n",
    "    # Print epoch information\n",
    "    if (e+1) % print_every == 0:\n",
    "        epochloop.write(f'Epoch {e+1}/{epochs} | Training Loss: {training_loss / len(training_loader):.3f} Training Accuracy: {training_accuracy / len(training_loader):.3f} | Val Loss: {validation_loss / len(validation_loader):.3f} Val Acc: {validation_accuracy / len(validation_loader):.3f}')\n",
    "        epochloop.update()\n",
    "\n",
    "    # Save model if validation loss decreases\n",
    "    if validation_loss / len(validation_loader) <= validation_loss_min:\n",
    "        torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        validation_loss_min = validation_loss / len(validation_loader)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        epochloop.write(f'[WARNING] Validation loss did not improve ({validation_loss_min:.3f} --> {validation_loss / len(validation_loader):.3f})')\n",
    "        es_trigger += 1\n",
    "\n",
    "    # Force early stop\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopping at Epoch-{e+1}')\n",
    "        # Update epochs history\n",
    "        history['epochs'] = e+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc032a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss over epochs for training and validation accuracy\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.plot(range(history['epochs']), history['training_accuracy'], label='Training Accuracy')\n",
    "plt.plot(range(history['epochs']), history['validation_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da32ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting data with unseen data which wasn't included in training\n",
    "# Test loop\n",
    "model.eval()\n",
    "\n",
    "# Metrics for testing\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Lists to store true target values and predicted values for later evaluation\n",
    "all_target = []\n",
    "all_predicted = []\n",
    "\n",
    "# tqdm progress bar for the test loop\n",
    "testloop = tqdm(testing_loader, leave=True, desc='Inference')\n",
    "with torch.no_grad():\n",
    "    for feature, target in testloop:\n",
    "        # Move data to the specified device (GPU or CPU)\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(feature)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        test_acc += acc.item()\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Extend lists with true and predicted values\n",
    "        all_target.extend(target.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Print overall accuracy and loss for the test set\n",
    "    print(f'Accuracy: {test_acc/len(testing_loader):.4f}, Loss: {test_loss/len(testing_loader):.4f}')\n",
    "\n",
    "# Print out classification report\n",
    "print(classification_report(all_predicted, all_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_predicted, all_target)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
